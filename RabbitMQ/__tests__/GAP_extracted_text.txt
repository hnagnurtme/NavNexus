============================================================
PDF TEXT EXTRACTION REPORT
============================================================
Source URL: https://sg.object.ncloudstorage.com/navnexus/GAP.pdf
Extraction Date: 2025-11-19 18:32:36
Total Pages: 17
Extracted Pages: 17
Language: en (confidence: 0.10)
File Size: 9,132,717 bytes
Average Text per Page: 4584 characters
============================================================

=== PAGE 1 ===
See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/384072941
Graphic Deep Reinforcement Learning for Dynamic Resource Allocation in
Space-Air-Ground Integrated Networks
ArticleÂ Â inÂ Â IEEE Journal on Selected Areas in Communications Â· September 2024
DOI: 10.1109/JSAC.2024.3460086
CITATIONS
8
READS
172
6 authors, including:
Yue Cai
The University of Sydney
5 PUBLICATIONSÂ Â Â 51 CITATIONSÂ Â Â 
SEE PROFILE
Peng Cheng
La Trobe University
149 PUBLICATIONSÂ Â Â 2,974 CITATIONSÂ Â Â 
SEE PROFILE
Wei Xiang
La Trobe University
505 PUBLICATIONSÂ Â Â 14,502 CITATIONSÂ Â Â 
SEE PROFILE
Branka Vucetic
The University of Sydney
889 PUBLICATIONSÂ Â Â 25,054 CITATIONSÂ Â Â 
SEE PROFILE
All content following this page was uploaded by Peng Cheng on 20 September 2024.
The user has requested enhancement of the downloaded file.

=== PAGE 2 ===
Graphic Deep Reinforcement Learning for Dynamic
Resource Allocation in Space-Air-Ground Integrated
Networks
Yue Cai, Peng Cheng, Zhuo Chen, Wei Xiang, Branka Vucetic, Life Fellow, IEEE and Yonghui Li, Fellow, IEEE
Abstractâ€”Space-Air-Ground integrated network (SAGIN) is a
crucial component of the 6G, enabling global and seamless communication coverage. This multi-layered communication system
integrates space, air, and terrestrial segments, each with computational capability, and also serves as a ubiquitous computing platform. An efficient task offloading and resource allocation scheme
is key in SAGIN to maximize resource utilization efficiency,
meeting the stringent quality of service (QoS) requirements for
different service types. In this paper, we introduce a dynamic
SAGIN model featuring diverse antenna configurations, two
timescale types, different channel models for each segment, and
dual service types. We formulate a problem of sequential decisionmaking task offloading and resource allocation. Our proposed
solution is an innovative online approach referred to as graphic
DRL (GDRL). This approach utilizes a graph neural network
(GNN)-based feature extraction network to identify the inherent
dependencies within the graphical structure of the states. We
design an action mapping network with an encoding scheme for
end-to-end generation of task offloading and resource allocation
decisions. Additionally, we incorporate meta-learning into GDRL
to swiftly adapt to rapid changes in key parameters of the
SAGIN environment, significantly reducing online deployment
complexity. Simulation results validate that our proposed GDRL
significantly outperforms state-of-the-art DRL approaches by
achieving the highest reward and lowest overall latency.
Index Termsâ€”Space-Air-Ground integrated network, online
resource allocation, task offloading, deep reinforcement learning
(DRL), Graph neural network.
I. INTRODUCTION
The Space-Air-Ground integrated network (SAGIN) is the
key enabler of the 6G network to achieve ubiquitous global
coverage and seamless connection [1]. This innovative framework spans space, air, and terrestrial domains, consisting of
interconnected low earth orbit (LEO) satellites, aerial platforms (e.g., unmanned aerial vehicles (UAVs) or high altitude
platform stations (HAPSs)), and ground-based networks [2].
In particular, the advancements in onboard processing and
manufacturing cost reduction have significantly evolved LEO
and aerial platforms from their traditional roles as mere transmission relays to fully-fledged computing platforms [3], [4].
Y. Cai, B. Vucetic and Y. Li are with the School of Electrical and Information Engineering, the University of Sydney, Australia (yue.cai@sydney.edu.au;
branka.vucetic@sydney.edu.au; yonghui.li@sydney.edu.au). P. Cheng is with
the Department of Computer Science and Information Technology, La
Trobe University, Australia, and also with the University of Sydney, Australia (p.cheng@latrobe.edu.au); Z. Chen is with Data61, CSIRO, Australia (drbearsyd@hotmail.com). W. Xiang is with the Department of Computer Science and Information Technology, La Trobe University, Australia
(w.xiang@latrobe.edu.au). The work was supported by the Australian Research Council (ARC) under Grant DP210103410 and Grant DP220101634.
(Corresponding Author: Peng Cheng)
This could provide a significant leap forward in a large number of emerging applications, such as hyperspectral sensing,
intelligent transportation systems (ITS), and massive Internet
of Things (IoT) [5]â€“[7].
Effective task offloading and resource allocation are critical
for dynamic network management in SAGIN [8]. Task offloading involves transferring data processing tasks from resourceconstrained nodes to more capable ones within the network,
such as from terrestrial devices to satellite or aerial platforms,
thereby enhancing processing speed and reducing latency [9].
For effective resource allocation strategies, they are essential
for managing limited or varying resources of each layer, ensuring bandwidth, computing power, and storage are efficiently
utilized to support a wide range of applications with diverse
quality-of-service (QoS) requirements [10], [11]. However,
performing efficient task offloading and resource allocation
is a challenging task. This is because SAGIN functioning in
a dynamic environment involves changing features such as
channel states, LEO and aerial platform locations, and total
available resources [12], [13]. This requires highly adaptive
and nearly real-time resource allocation approaches to capture
and closely follow the variations of these features [14], [15].
A. Related Work
To address the resource allocation and task offloading problems in SAGIN, many approaches have been proposed, and
they can be generally categorized as optimization-based and
deep reinforcement learning (DRL)-based. We first elaborate
on the functionality of the optimization-based approaches
[16]â€“[21]. In [16], a distributed decision-making scheme is
introduced, where Lyapunov optimization is employed to break
down the main problem into several manageable sub-problems
for each satellite. Another study [17] utilizes sequential fractional programming and Lagrangian dual decomposition to
solve the computation offloading problem by minimizing the
total energy consumption of Internet of Things (IoT) devices.
In [18], the alternating optimization (AO) method is utilized
to solve the problem of minimizing the energy consumption
of LEO satellites by decomposing it into two sub-problems.
Further research [19] employs the block successive upperbound minimization (BSUM) method for optimal offloading
decisions in mobile edge computing (MEC) enabled SAGIN.
The goal is to minimize user-experienced latency while considering the energy consumption constraints of UAV and LEO.
The same method is also applied in [20] to solve the UAV
trajectory planning, resource allocation, and task offloading

=== PAGE 3 ===
problems, aiming to minimize the energy consumption of both
IoT devices and UAVs. Lastly, in [21], the authors exploit
block coordinate descent and successive convex approximation
to solve SAGINâ€™s UAV position optimization, resource allocation, and task offloading problems. However, these approaches
cannot adapt to the dynamically changing SAGIN environment
and must resolve the optimization problems frequently when
the network parameters change. Furthermore, the high computational complexity involved prevents the online deployment of
these methods, especially for a larger-scale network.
To address the aforementioned problem of optimizationbased approaches, DRL-based approaches have been proposed
in [22]â€“[26]. The key benefits of DRL lie in its ability to
directly map the varying network parameters to task offloading and resource allocation decisions by interacting with the
environment and receiving corresponding rewards, enabling
online resource management. In [22], a hybrid centralized
and decentralized deep-Q network (DQN) method is proposed to minimize the total learning cost in a cloud serverequipped satellite network. In [23], the authors introduce a
heuristic method for resource allocation and task offloading
for UAV edge servers. With the assistance of DQN, the
same heuristic method is also employed to address the task
offloading problem for SAGIN. The study in [24] presented an
attention-enabled proximal policy optimization (PPO) method
to generate offloading decisions. It utilizes UAVs to collect
tasks from IoT devices and perform task offloading. In [25], the
task offloading and resource allocation problem is divided into
two separate subproblems. An attention-enabled multi-agent
PPO is used to solve the task offloading while transforming
the resource allocation into a convex problem. In [26], the
authors propose a slicing-based task offloading framework
with a dynamically changing slicing window length in SpaceAir-Ground integrated vehicular networks (SAGIVN). This
involves using deep DQN for task scheduling, convex optimization for resource allocation, and a heuristic method to
adjust the slicing window length.
However, these conventional DRL-based approaches have
several limitations. Firstly, the SAGIN model in [23], [26] only
considers a single timescale for all services, single antenna
configurations, and a common channel model for all three
segments. However, different types of services require varying
timescales, and the distinct features of each segment should be
explored. Secondly, these methods cannot effectively capture
the dependencies within environmental state data featured by
topological structures. This leads to slow convergence and
performance degradation in terms of the reward function [27].
Thirdly, these DRL-based approaches generate task offloading
(discrete) and resource allocation decisions (continuous) separately. The original optimization problems are typically divided
into two subproblems: one solved by DRL-based methods and
the other by heuristic or optimization-based methods. This
division increases the overall computational complexity and
results in potential performance loss. Lastly, these approaches
assume minor differences between deployment and training
environments. In SAGIN, however, critical parameters like
initial total resources may drastically change due to LEO and
aerial platform movements, resulting in inference performance
Fig. 1: System architecture of the Space-Air-Ground integrated
network (SAGIN).
degradation.
B. Main Contribution
In this paper, we consider a dynamic SAGIN model with
different antenna configurations, two kinds of timescales, different channel models for each segment, and two service types:
delay-sensitive (DS) and delay-tolerant (DT). On this basis,
we formulate a problem of sequential decision-making task
offloading and resource allocation that includes a mixed action
space (both discrete and continuous). To address this problem, we propose a new online resource allocation approach
referred to as graphic DRL (GDRL). We design a graph neural
network (GNN) referred to as a graphic feature extraction
network (GFEN) to capture the inherent dependencies within
the topological structure of the states. It can directly process
graph-structured SAGIN states without the need to transfer
the data into one dedicated form, reducing the complexity
of data preprocessing. Compared to conventional DRL-based
approaches, it eliminates spatial information loss (e.g., interconnection between segments), which is critical for identifying
spatial correlations among segments. A new action mapping
network (AMN) and encoding scheme are proposed for the
simultaneous and end-to-end generation of both discrete and
continuous actions. In addition, we incorporate meta-learning
into GDRL to enable agile adaptation to the fast variation of
critical parameters of SAGIN environment, which significantly
reduces the online deployment complexity.
The main contributions of this paper are summarized as
follows.
1) We propose a SAGIN model that caters to both DS
and DT-type services and distinct features of all three
segments. This reflects a more realistic scenario with a
broader spectrum of service types.
2) We propose a GNN-based GFEN to capture spatial correlations in the graphical structure of SAGIN states, where
each segment is treated as a node, and their connections
are edges. GFEN can adapt to changes in graph topology
caused by segment movement, ensuring continuous and
accurate feature extraction.
3) We propose an action mapping network (AMN) to enable
the end-to-end generation of both task offloading and re-

=== PAGE 4 ===
source allocation decisions. AMN, along with the encoding scheme, can decrease the computational complexity
and achieve fast online decision inference.
4) We leverage a model agnostic meta-learning (MAML)
to construct a more efficient GDRL, which enables fast
adaptation of GDRL to fast-changing environments only
with the aid of several gradient updates.
The rest of this paper is formulated as follows. In Section II,
we elaborate on the system model of SAGIN. We formulate
the task offloading and resource allocation problem in Section
III. Then, in Section IV, we describe the proposed GDRL in
detail and elaborate on the functionality of each part of GDRL.
In Section V, the integration process of MAML in GDRL
is introduced. We then demonstrate the simulation results in
Section VI. Finally, in Section VII, we conclude the paper.
II. SYSTEM MODEL
A. Network Model
We consider a Space-Air-Ground integrated network (SAGIN) shown in Fig. 1. It consists of three segments: the
space segment (low earth orbit (LEO) network/constellation),
the air segment (high altitude platform stations (HAPS) network), and the ground segment (user equipment (UE)). UEs,
equipped with the global navigation satellite system (GNSS),
can connect directly to both LEO and HPAS. For simplicity,
we refer to UE as user throughout this paper. We define the
set of LEOs, HAPSs, and users as l âˆˆL = {1, Â· Â· Â· , L}, n âˆˆ
N = {1, Â· Â· Â· , N}, and U = {1, Â· Â· Â· , U}, respectively. Each
user can generate two types of tasks: DS with stringent latency requirements and DT with flexible latency requirements
denoted by ÂµD and ÂµT , respectively. A DS task generated
by user u is represented as ÂµD
u . To prevent user interference,
we implement orthogonal frequency division multiple access
(OFDMA). Throughout the network, a time-slotted structure
is adopted, where the timeline is divided into two kinds of
timescales, the long timescale (LT) and the short timescale
(ST). The LT is represented by T = {1, 2, . . . , T}. Each LT
is further subdivided into S ST denoted by S = {1, 2, . . . , S}.
The DS and DT tasks may arrive at the start of each ST and
LT, respectively.
In this paper, the resources to be allocated include virtual
machines (VMs) and subchannels. VMs represent the computational resources for task processing, while subchannels
represent the frequency band allocated for the transmission
between the user and server (LEO or HAPS). The total
available frequency resource is divided into K subchannels.
We use an indicator au âˆˆ[0, 1] to represent the portion of
subchannels allocated to user u.
B. Channel Model
The channels involved in this paper, shown in Fig. 1, include
the ground-to-air channel(â¶), ground-to-space channel(â·), airto-space channel(â¸), inter-satellite link (ISL)(â¹), and interHAPS link (IHL âº). The antenna deployment for LEO and
HAPSâ€™ is also shown in Fig. 1. Specifically, we assume that
the HAPS is equipped with a downward-facing uniform planar
array (UPA) consisting of M 1 = M 1
x Ã— M 1
y antennas and an
upward-facing single antenna. The LEO satellite is equipped
with UPA composed of M 2 = M 2
x Ã— M 2
y antennas, where
M 2
x denotes the quantity along the x-axis while M 2
y indicates
those along the y-axis.
The channel coefficient for â¶and â·is represented as
hn/l
u,k =
v
u
u
tGn/l
u

c
4Ï€dn/l
u f l/n
k
!
wn/l
u gn/l
u,k Â· nn/l
u ,
(1)
where hn
u,k represents the channel coefficient for â¶, hl
u,k
represents the channel coefficient for â·, and Gn/l
u
represents
the total antenna gain, a product of the userâ€™s antenna gain
and that of HAPS n or LEO l. The term dn/l
u
is the distance
between user u and HAPS n or LEO l, wn/l
u
refers to environmental effects-induced attenuation,gn/l
u,k represents the smallscale fading which follows Rician distribution with Rician
factor Kn/l
u
, and nn/l
u
âˆˆCMÃ—1 is the array response vector
written as
nn/l
u
= aM 1/2
x

sin Î¸y
n/l,u cos Î¸x
n/l,u

âŠ—aMy

cos Î¸y
n/l,u

,
(2)
with
aM 1/2
âˆ—
(x) =
1
q
M 1/2
âˆ—

1, e
âˆ’j 2Ï€cdâˆ—
fn/l
k
x
, Â· Â· Â· , e
âˆ’j 2Ï€cdâˆ—
fkl (M 1/2
âˆ—
âˆ’1)x
!
,
(3)
where Î¸n/l,u
=

Î¸x
n/l,u, Î¸y
n/l,u

denotes the angles-ofdeparture (AoDs) for downlink (DL) transmission from LEO
l or HAPS n to user u or the angles-of-arrival (AoAs) for UL
transmission, âˆ—represents x or y axis, and dâˆ—is the antenna
spacing along either x or y axis.
All other channels (â¸,â¹,âº) are quasi-vacuum or vacuum
channels, and can be modeled as additive white Gaussian noise
(AWGN) ones.
C. Transmission Rate Model
The data transmission rate for task ÂµT
u on channel â¶and â·
can be represented by
Rn/l
ÂµT
u ,k = log2

1 +
pÂµT
u hn/l
u,k(hn/l
u,k)H
Ïƒ2
!
,
(4)
where pÂµT
u is the transmit power of task ÂµT generated by user
u, hn/l
u,k represents the channel coefficient in Eq. (1), and Ïƒ2
denotes the variance of AWGN.
For task ÂµD, the data transmission rate on channel â¶and
â·can be expressed as [28]
Rn/l
ÂµD
u ,k = B
ln 2

ln
ï£®
ï£°1 +
pn/l
ÂµD
u hn/l
u,k(hn/l
u,k)H
Ïƒ2B
ï£¹
ï£»
âˆ’
s
1
TfB f âˆ’1
Q

Ïµd
ÂµD
u
 
,
(5)
where Ïµd
ÂµD
u is the decoding error probability, B is bandwidth
per subcarrier, Tf is the time-length of a ST, f âˆ’1
Q
is the inverse
Q function, and pÂµD
u represents the user uâ€™s transmit power for
task ÂµD.

=== PAGE 5 ===
D. Computing Model
A
user-generated
computation
task
Tu(t)
=
{ou(t), su(t), Ï…u(t), Î¹u(t)}, is defined by four parameters:
the required computing intensity in CPU cycles per bit
ou(t), its size in bits su(t), the size after processing in
bits Ï…u(t), and its delay constraint in seconds Î¹u(t). Each
user has a probability Ïµu of generating one such task at
each timestep t. These tasks can be processed locally or
offloaded to LEO or HAPS. Upon receiving tasks from
users, LEO/HAPS has the option to process them locally or
offload them to another LEO/HAPS. To illustrate connections
between users, HAPSs, and LEOs, we use an adjacency
matrix E âˆˆ{0, 1}(U+L+N)Ã—(U+L+N) where 1 â‰¤row â‰¤U
represent user nodes, U + 1 â‰¤row â‰¤U + L represent LEO
nodes and U + L + 1 â‰¤row â‰¤U + L + N represent HAPS
nodes. For example, if user node u connects with LEO l, then
eu,U+l = 1. If it connects with HAPS n, then eu,U+L+n = 1.
We assume that a task can either be processed locally, offloaded to LEO/HAPS, or dropped by the user directly. Based
on their accessibility by users, LEO/HAPS can be categorized as accessible and non-accessible. Accessible LEO/HAPS
can directly receive the offloaded tasks from users, whereas
non-accessible ones only receive these tasks via connected
LEO/HAPS.
1) Task Offloading Decisions: Task offloading decisions are
made based on three indicators.
â€¢ Local computing indicator Î±u(t)
âˆˆ
{0, 1}, where
Î±u(t) = 1 indicates that the task generated by user u
is processed locally.
â€¢ Vertical task offloading indicator Î²u,j(t) âˆˆ{0, 1}, where
Î²u,j(t) = 1 represents that the task generated by user u
is offloaded to an edge server. Here 1 â‰¤j â‰¤L indicates
offloading to LEO and L + 1 â‰¤j â‰¤L + N to HAPS.
Note that j is determined by E.
â€¢ Horizontal task offloading indicator Î·u,j,k(t) âˆˆ{0, 1},
where Î·u,j,k(t) = 1 indicates that a task generated by
user u is first offloaded to server j and then to server k.
Here, 1 â‰¤k â‰¤L and L + 1 â‰¤k â‰¤L + N represent
offloading to LEO and HAPS, respectively.
Clearly, based on the above three indicators, we have the
following constraint
Î±u(t) +
L+N
X
j=1
Î²u,j(t) +
L+N
X
j=1
L+N+1
X
k=1
eu,jej,kÎ·u,j,k(t) â‰¤1. (6)
In this paper, we only consider the complete offloading of each
task. This is because partial offloading is not suitable for the
task types considered in this paper. Specifically, the packet
size for DS is very small, which makes partial offloading
unnecessary for DS. For the DT task, given the non-stringent
delay requirements, it is more politic to prioritize the DS
over DT in terms of UE computational resource reservation.
Therefore, partial offloading for DT is usually regarded as
inefficient.
2) Latency: Latency from task offloading includes propagation, processing, and handover. User requests are assumed
to be processed immediately upon arrival at the corresponding
LEO/HAPS server. If resources are insufficient, the DS request
will be dropped. In contrast, the DT request will be treated as
a new request at the next timeslot, with a higher priority value
and a more stringent delay requirement compared with the
previous DT request. The propagation latency for user u can
be calculated as
Lu
p(t) =
ï£±
ï£´
ï£²
ï£´
ï£³
0,
Î±u(t) = 1,
su
auKRu,k ,
Î²u,j(t) = 1,
su
auKRu,k + dj,k
c ,
Î·u,j,k(t) = 1,
(7)
where Ru,k is chosen from the previously described transmission rate model based on specific user types and offloading decisions. If Î±u(t) = 1, the task is processed locally
without propagation latency. When Î²u,j(t) = 1, user uâ€™s
task is offloaded to edge server j for processing. However,
if Î·u,j,k(t) = 1, the task first goes to edge server j, then to
server k. Note that dj,k is the distance between edge server j
and k.
The processing latency for user u can be calculated as
Lu
c (t) =
(
ou(t)
Cu ,
Î±u(t) = 1,
ou(t)
cu
j (t)âˆ—Cjâˆ—Ï„ (max) ,
Î²u,j(t)/Î·u,j,k(t) = 1,
(8)
where cu
j (t) denotes the portion of the total number of VM
instances allocated by server j to a task from user u at timestep
t, Cj represents the total number of VM instances in server
j, Ï„ (max) is the maximum CPU cycle (Hz/s) of each VM
instance, and Cu is the computational capability of user u.
After computation, the result is sent from LEO/HAPS to the
user. The result transfer latency can be calculated as
Lu
h(t) =
ï£±
ï£´
ï£²
ï£´
ï£³
0,
Î±u(t) = 1,
vu
auKRu,k ,
Î²u,j(t) = 1,
vu
auKRu,k + dj,k
c ,
Î·u,j,k(t) = 1,
(9)
Then, the total latency for computational offloading is calculated as Lu
o(t) = Lu
p(t) + Lu
c (t) + Lu
h(t).
III. PROBLEM FORMATION
In this section, we formulate an optimization problem
that aims to find the offloading decision triple {Î±
âˆˆ
{0, 1}UÃ—1, Î² âˆˆ{0, 1}UÃ—(N+L), Î· âˆˆ{0, 1}UÃ—(N+L)Ã—(N+L)},
and allocate computational resources C âˆˆRUÃ—(N+L) and
portion of subchannels allocated a âˆˆRU. The goal is to
minimize latency in task offloading. Given different latency
requirements for ÂµD and ÂµT , we split the total latency into two
parts, namely latency for ÂµD and latency for ÂµT . Specifically,
the latency for ÂµD can be calculated as
LÂµD
u
o (t) = ÎºD

LÂµD
u
p (t) + LÂµD
u
c
(t) + LÂµD
u
h (t)

,
(10)
where ÎºD represents the priority weight for task ÂµD. For ÂµT ,
the latency can be calculated as
t mod S Ì¸= 0 :
LÂµT
u
o (t) = 0,
t mod S = 0 :
LÂµT
u
o (t) = ÎºT

LÂµT
u
p (t) + LÂµT
u
c (t) + LÂµT
u
h (t)

,
(11)

=== PAGE 6 ===
where t mod S = 0 represents the start of each LT and ÎºT
represents the priority weight for task ÂµT . Consequently, the
total latency can be represented as
Lo(t) =
X
uâˆˆU
ÎµuLÂµT
u
o (t) + (1 âˆ’Îµu) LÂµD
u
o (t),
(12)
where Îµu
= 1 indicates that user u generates task ÂµT .
Accordingly, the optimization problem can be formulated as
P :
min
Î±,Î²,Î·,C,a lim
T â†’âˆ
1
T
t=ST
X
t=1
Lo(t)
(13)
X
uâˆˆU
cu
j (t) â‰¤Cj,
(14a)
X
u
au,k â‰¤K,
(14b)
Lu
o(t) â‰¤Î¹u(t),
(14c)
Eq. (6)
(14d)
where Cj denotes the total VM instances in server j. Constraint Eq. (14a) and Eq. (14b) ensure that allocated resources
are within the limits. Constraint Eq. (14c) guarantees the satisfaction of the latency requirement. Lastly, constraint Eq. (14d)
states that a task can either be accepted or dropped. If accepted,
only one of the indicators can equal to one(Î± = 1 or Î² = 1
or Î³ = 1).
This formulated problem P is difficult to solve due to the
following reasons.
(C1) P is an NP-hard long-term mixed-integer nonlinear programming (MINLP) with discrete variables {Î±, Î², Î³} and
continuous variables {C, a}. As a sequential decisionmaking problem, the solution set chosen at the current
timestep will influence the next. Therefore, traditional
optimization approaches [29], [30] cannot solve this
problem due to the dependencies among solution sets at
different timesteps.
(C2) State-of-the-art DRL-based approaches still cannot solve
P due to three main reasons. 1) These approaches lack the
ability to extract the inherent complex spatial correlations
within the topological structure of SAGIN states. Thus
these approaches can not effectively learn the mapping
between graphical-structured SAGIN states and actions,
resulting in slow convergence to suboptimal solutions and
performance degradation in reward function. 2) These
approaches generate discrete (task offloading) and continuous (resource allocation) separately, and a separation
of problem P is required. This leads to an increase
in the computational complexity and performance loss.
Furthermore, DRL-based approaches cannot generate a
Q-value for each discrete action due to the large action
space created by discrete offloading decisions. 3) The
dynamic nature of the SAGIN environment is characterized by continuous variation in crucial system parameters.
In addition, the complex interactions among space, air,
and ground segments lead to dynamic underlying features
related to task generation and all three segments. Given
that DRL-based approaches rely on small differences
between testing and training environments, they cannot
be deployed online due to high retraining computational
complexity.
IV. THE PROPOSED GDRL
A. Motivation
To address the aforementioned challenges, we propose a new
method referred to as GDRL. The core of GDRL is to utilize
a GNN-based GFEN to capture the spatial correlations within
graphically structured states. It processes the graphical data
directly, eliminating the need for loss-incurring vectorization
transformation.
In the sequel, we first discuss how the design of GDRL
meets the challenges before we delineate on its major components. To address (C1), we propose a model-free method
to directly generate offloading and resource allocation decisions based on current observations. Also, we separate the
environmental states into static (graph-structured) and dynamic
ones (non-graph-structured); the latter is influenced by the
solution set at the previous timestep, while the former does
not. Each state is processed by a uniquely tailored neural
network. Furthermore, we develop a reward function to handle
constraints Eq. (14a), Eq. (14b), Eq. (14c), Eq. (14d). To
tackle (C2), firstly, we design GFEN to capture the spatial
correlations that inherently exist in graphic-structured states.
Secondly, we propose a new encoding scheme and AMN to
enable the direct end-to-end generation of both discrete and
continuous actions. Finally, we integrate meta-learning into
GDRL to allow low complexity fast adaptation to critical
parameter changes in SAGIN.
The proposed GDRL is shown in Fig. 2. It consists of one
auxiliary part (Part 1), three main parts (Part 2,3,4), and a reply
buffer to store all the transactions and calculated rewards. In
Part 1, the DRL agent interacts with the environment using
actions generated by the action mapping network to calculate
final rewards. In Part 2, environmental states are divided into
static and dynamic ones based on whether or not influence is
imposed from previous actions. We denote Ssta and Sdyn as
static states and dynamic states, respectively. These states are
then fed into the GFEN for feature extraction. Part 3 inputs
the extracted features of states into the LAGN to generate
latent actions for both discrete and continuous actions. These
latent actions are then fed into the action mapping network in
Part 4 to generate offloading and resource allocation decisions.
Subsequent subsections provide detailed explanations of each
part of GDRL.
B. The Component of GDRL
1) Reward Calculation (Part 1 in Fig. 2): In this part, to
reflect the priorities of different tasks, delay requirements, and
total resource constraints, we formulate the reward function
as Eq. (15), differentiating whether the latency requirement is
met (Lu
o(t) â‰¤Î¹u(t)) or not (Lu
o(t) > Î¹u(t)). Here, bp is a
large value penalty term that is tuned during training, and the
term multiplied by Î»1, Î»2, and Î»3 represents the constraint
for total available computational resources, the total available
bandwidth, and the latency, respectively. Specifically,Î»1(Cj âˆ’

=== PAGE 7 ===
Fig. 2: The architecture of the proposed GDRL.
R(t) =

Î»1
P
j(Cj âˆ’P
u cu
j (t)) + Î»2(K âˆ’P
u au,k) + Î»3(P
u Îº(Î¹u(t) âˆ’Lu
o(t))),
u âˆˆ{Lu
o(t) â‰¤Î¹u(t)},
Î»1
P
j(Cj âˆ’P
u cu
j (t)) + Î»2(K âˆ’P
u au,k) âˆ’P
u bp,
u âˆˆ{Lu
o(t) > Î¹u(t)},
(15)
P
uâˆˆU cu
i (t)) is the penalty term for the total computational
resources constraint P
uâˆˆU cu
j (t) â‰¤Cj, Î»2(K âˆ’P
u au,k) is
the penalty term for the total frequency constraint P
u au,k â‰¤
K, and Î»3(P
u Îº(Î¹u(t)âˆ’Lu
o(t))) is the reward term if the final
achieved latency Lu
o(t) is less than the latency constraint Î¹u(t).
When the latency constraint is not satisfied Lu
o(t) > Î¹u(t), a
large penalty term âˆ’bp is added to the reward. Note that we
only count the tasks that have been accepted into the network.
2) State Separation (Part 2 in Fig. 2): Static states include the location information of LEO locl, HAPS locn and
user locu, the connection status E, initial computation Cinit
n/l
resources of LEO and HAPS, and the initial computation
capacity Cinit
u
of user u. Dynamic states include tasks Tu(t),
remaining computation resources Cn/l(t) of LEO and HAPS,
total available subchannels number K(t), and the remaining
computation resources Cu(t) of user u. Tasks Tu(t) fall under
dynamic states as they are impacted by offloading decisions.
For example, for ÂµT , a rejected task (Î± = 0, Î² = 0, Î³ = 0)
is assigned a new priority value to be processed at the next
timestep. If the delay ÂµT exceeds the threshold, the task
will be dropped. In contrast, ÂµD is immediately dropped due
to stringent latency requirements. After state separation, the
GFEN receives Ssta and Sdyn to generate latent actions.
Fig. 3 illustrates the structure of GFEN, which includes
an encoder and a decoder. The encoder directly processes
static states Ssta. Its output, combined with dynamic states
Sdyn, is fed into the decoder for latent action generation.
Here, we leverage the graph convolutional network (GCN) to
construct the encoder due to its efficiency in extracting features
from graph-structured data like static states Ssta. We define an
undirected graph G = (V, E) where V = {U, L, N} represents
nodes, and E represents edges. Each user, LEO, and HAPS
are treated as nodes in the graph G. The connection status E
serves as an adjacency matrix, and other parts of Ssta act as
Fig. 3: The structure of the proposed GFEN.
node feature X =

li, Cinit
i

i âˆˆR(U+L+N)Ã—2 with each type
of feature represented by x âˆˆRU+L+N. The degree matrix D
is a diagonal matrix and can be represented as
D =

di,i,
i = j,
0,
i Ì¸= j,
(16)
where
di,i =
X
j
ei,j.
(17)
Accordingly, the multiplication of x with a filter gÎ¸
=
diag(Î¸) âˆˆRU+L+N in the Fourier domain can be represented
as
gÎ¸ â‹†x = UgÎ¸UâŠ¤x,
(18)
where U is the matrix of eigenvectors for the normalized
graph Laplacian matrix L and can be obtained from the
decomposition L
L = I(U+L+N) âˆ’Dâˆ’1
2 EDâˆ’1
2 = UÎ›UâŠ¤,
(19)

=== PAGE 8 ===
where Î› denotes the diagonal matrix of eigenvalues. According to [31], gÎ¸ is a function of eigenvalues of L, and can be
represented as gÎ¸(Î›). The calculation of Eq. (19) incurs a high
complexity (O((U + L + N)2)). Consequently, the authors in
[32] proposed to approximate gÎ¸(Î›) as a truncated expansion
in terms of Chebyshev polynomials Tk(x) up to K-th order,
which is
gÎ¸â€²(Î›) â‰ˆ
K
X
k=0
Î¸â€²
kTk( ËœÎ›),
(20)
where Î¸â€²
k represents the Chebyshev coefficients. In addition,
ËœÎ› =
2
eigenmax
Î› âˆ’I(U+L+N),
(21)
denotes a re-scaled variant of Î›, where eigenmax is the largest
eigenvalue of L. The Chebyshev polynomials are defined as
Tk(x) = 2xTkâˆ’1(x) âˆ’Tkâˆ’2(x),
T0(x) = 1, T1(x) = x.
(22)
Replacing gÎ¸ in Eq. (18) with gâ€²
Î¸ in Eq. (20), we can derive
gÎ¸â€² âˆ—x â‰ˆ
K
X
k=0
Î¸â€²
kTk(ËœL)x,
(23)
where ËœL represents the re-scaled version of L with
ËœL =
2
eigenmax
L âˆ’I(U+L+N).
(24)
By setting K = 1 and eigenmax = 2, Eq. (23) can be simplified
as
gÎ¸â€² âˆ—x â‰ˆÎ¸â€²
0x âˆ’Î¸â€²
1Dâˆ’1
2 EDâˆ’1
2 x.
(25)
Constraining the number of parameters further can be beneficial for addressing overfitting and minimizing the number of
operations. Eq. (25) can be simplified into
gÎ¸ â‹†x â‰ˆÎ¸

IU+L+N + Dâˆ’1
2 EDâˆ’1
2

,
(26)
where Î¸ = Î¸â€²
0 = âˆ’Î¸â€²
1. For X and GCN with F filters, Eq. (26)
can be generalized into matrix representation as
O =

IU+L+N + Dâˆ’1
2 EDâˆ’1
2

XÎ˜,
(27)
where Î˜ âˆˆR2Ã—F represents the matrix of filter parameters,
and O âˆˆR(U+L+N)Ã—F represents the graph convolution
output. We then forward the output O and dynamic states Sdyn
to the decoder to extract features of states further. Specifically,
the input to the decoder network is Xdyn = {T âˆˆRUÃ—4, C âˆˆ
R(L+N)Ã—1, K âˆˆR(L+N)Ã—1, O âˆˆR(U+L+N)Ã—F }, and a
feedforward neural network (FNN) is leveraged to build the
decoder network. We vectorized each component of Xdyn into
T âˆˆR4U, C âˆˆR(L+N), K âˆˆR(L+N), O âˆˆRF (U+L+N)),
and concatenate them together. The hidden state output for
the decoder can be represented as
Hdec = ReLu(XdynÎ˜dec),
(28)
where Î˜dec âˆˆRdim(X)Ã—Dh represents the matrix of FNN
hidden state parameters, and Dh represents the dimension of
the hidden state.
Fig. 4: The structure of the proposed LAGN.
3) Latent Action Generation Network (Part 3 in Fig. 2):
After extracting features of states in Part 1, we fed the GFEN
output into the latent action generation network (LAGN) to
generate latent action. Fig. 4 shows the structure of the
proposed LAGN, which outputs two components: discrete and
continuous latent actions. Note that the term â€œdiscrete latent
actionâ€ only refers to the segment used for generating the
final discrete action. The latent action generation problem
can be formulated as a Markov decision process (MDP)
problem and represented by a tuple {S, A, M, V, g, Ï0}, where
A = {ac, ad} represents the latent action set consisting
of continuous (ac) and discrete (ad) actions. The transition
probability distribution is denoted by M : S Ã— A Ã— S â†’R,
while V represents reward function. g âˆˆ(0, 1) represents
the discount factor, and Ï0 : S â†’R represents initial state
distribution. Note that the state set S is the output received
from the GFEN. We use Gaussian distribution as the main
policy distribution, with LAGN directly outputting its mean
and variance. The exact latent actions can be sampled from the
Gaussian distribution. Specifically, we use a modified version
of trust region policy optimization (TRPO) to build the LAGN.
There are two reasons for choosing TRPO as the basic DRL
algorithm. Firstly, TRPOâ€™s policy update guarantees monotonic
improvement, a feature absent in other DRL methods. This
feature ensures that each update step yields an improved policy
over the last. Secondly, TRPO implements the advantage
function
AÏ€(o(t), ac(t), ad(t)) = QÏ€(o(t), ac(t), ad(t)) âˆ’VÏ€(o(t)),
(29)
which offers a more stable policy update than other policy
gradient methods. Here, Ï€ : A Ã— S â†’[0, 1] represents
the policy, o(t) the output obtained from the GFEN, ac(t)
the continuous latent action, ad(t) the discrete latent action,
QÏ€(o(t), ac(t), ad(t)) the Q-value function, and VÏ€(o(t))
the state value function. For simplicity, we write a(t) =
[ac(t), ad(t)]. In addition, the Q-value function QÏ€(o(t), a(t))
can be represented as
QÏ€(o(t), a(t)) = Eo(t+1),a(t+1),...
" âˆ
X
l=0
glÎ¶(o(t + l))
#
,
(30)
where g represents the discount factor, l the time lag, and
(o(t + 1), a(t + 1), . . .) the trajectory of states and actions se-

=== PAGE 9 ===
lected after (o(t), a(t)). With the definition of AÏ€(o(t), a(t)),
we can now define the advantage of one policy over another.
Here, we denote Ï€old as old policy and Ï€new as new policy after
an update. The advantage of Ï€old over Ï€new can be defined as
Î¾(Ï€new) = Î¾(Ï€old) + Eo0,a0,Â·Â·Â·âˆ¼Ï€new
" âˆ
X
t=0
gtAÏ€old (o(t), a(t))
#
,
(31)
where o0, a0, Â· Â· Â· âˆ¼Ï€new represents the trajectory of states and
actions selected following the policy Ï€new, and
Î¾(Ï€) = Eo0,a0,...
" âˆ
X
t=0
gtÎ¶(o(t))
#
.
(32)
Expanding the expectation term of Eq. (31), we can get
Î¾(Ï€new) = Î¾(Ï€old) +
X
o
Î·Ï€new(o)
X
a
Ï€new(a | o)AÏ€old(o, a),
(33)
with
Î·Ï€(o) =
âˆ
X
t=0
gtP (o(t) = o | Ï€) ,
(34)
where Î·Ï€(o) represents the discounted transition probability
distribution. Note that the subscript of Ï€ has been omitted for
simplification. We set the objective function as
LÏ€old(Ï€new) =Î¾(Ï€old)+
X
o
Î·Ï€old(o)(
X
a
Ï€new(a | o)AÏ€old(o, a) + E (o)),
(35)
where E (o) represents entropy loss and is formulated as
E (o) = âˆ’
X
ac
Ï€new (ac | o) ln Ï€new (ac | o)
âˆ’
X
ad
Ï€new (ad | o) ln Ï€new (ad | o) .
(36)
There are two differences between the objective functions
Eq. (35) and Eq. (33). Firstly, Eq. (35) has an additional
term E (o). The term E (o) is introduced to the objective
function to encourage exploration. Secondly, Eq. (35) uses Î·Ï€old
to approximate Î·Ï€new in Eq. (33) with the goal of reducing the
computational complexity. This approximation is only valid if
the difference between Î·Ï€old and Î·Ï€new, measured by KullbackLeibler (KL) divergence, is within a trust region. Accordingly,
the maximization problem for TRPO can be formulated as
maximize
Ï€new
LÏ€old(Ï€new)
subject to DKL (Ï€new(Â· | o)âˆ¥Ï€old(Â· | o)) â‰¤Î´,
(37)
where DKL represents the KL divergence and Î´ represents
the trust region constraint. As the policy is controlled by the
parameters of the policy network, problem Eq. (37) can be
simplified using linear and quadratic approximation for the objective function and KL divergence, respectively. Accordingly,
the problem can be simplified as
maximize
Î¸â€²
zT (Î¸p
new âˆ’Î¸p
old)
subject to 1
2 (Î¸p
new âˆ’Î¸p
old)T CÎ¸p (Î¸p
new âˆ’Î¸p
old) â‰¤Î´,
(38)
where Î¸p
new/old represents the parameters of the policy network,
CÎ¸p
new/old represents the Hessian matrix of the KL divergence,
and z represents the gradient of LÏ€old(Ï€new) with respect to
Î¸p. We can solve Eq. (38) using Lagrangian duality, and the
parameter update rule for the policy network is
Î¸p
new = Î¸p
old + Îº
s
2Î´
zT Câˆ’1
Î¸p zCâˆ’1
Î¸p z,
(39)
where Îº represents the step size which can be obtained from
line search. Furthermore, the update rule for the value network
is
Ï•new = arg min
Ï•
1
ST
ST
X
t=0
(VÏ• (o(t)) âˆ’Î¶(o(t)))2 ,
(40)
where Ï• represents the parameters of the TRPO value network,
and Î¶(o(t)) represents the true reward.
4) Action Mapping Network (Part 4 in Fig. 2): After the
generation of latent actions in Part 2, we forward the output of
the LAGN to the AMN. The structure of the proposed AMN
is shown in Fig. 5. It can be seen that the AMN employs
a long-short-term memory (LSTM) network to handle input.
Specifically, LSTM can store long-term dependencies using
memory cells and three types of gates: input gate, forget gate,
and output gate, which can be presented as
Ïˆi(t) = Î´(Î˜l
ih(t âˆ’1) + Î¸l
ix(t) + bi),
Ïˆf(t) = Î´(Î˜l
fh(t âˆ’1) + Î¸l
fx(t) + bf),
Ïˆo(t) = Î´(Î˜l
oh(t âˆ’1) + Î¸l
ox(t) + bo),
(41)
respectively.
Here,
Ïƒ
represents
the
sigmoid
function,
{Î˜l
i, Î¸l
i, bi}, {Î˜l
f, Î¸l
f, bf}, {Î˜l
o and Î¸l
o, bo} the learnable parameters for LSTM input gate, forget gate and output gate, and
h the hidden state. Furthermore, memory cells are represented
as
Ïˆi(t) = tanh(Î˜l
ch(t âˆ’1) + Î¸l
cx(t) + bc),
c(t) = Ïˆf(t) âŠ™c(t âˆ’1) + Ïˆi(t) âŠ™Ïˆc(t),
(42)
where Ïˆc(t) represents the input cell, c(t) the output cell, and
{Î˜l
c, Î¸l
c, bc} the learnable parameters for LSTM input cell.
Based on Eq. (41) and Eq. (42), the hidden state h(u) can be
formulated as
h(t) = Ïˆo(t) âŠ™c(t).
(43)
The last hidden state h(tact) is then treated as the input of the
next FNN network.
AMN consists of two parts: one for discrete action generation and one for continuous action generation. We distinguish
them as discrete AMN and continuous AMN, respectively. For
continuous AMN, the encoder directly generates the portion of
total resources allocated to each user.
For discrete AMN, the encoder output could be either
discrete action (Î±, Î², Î·) or probability associated with each
available discrete action. However, both choices are impractical, in our case. The former suffers from prohibitively high
complexity in addition to leading to impractical actions which
do not satisfy constraint Eq. (6). For the latter, it suffers from a
high complexity as the possible combinations of these actions
is very large. In this paper, we adopt the second approach

=== PAGE 10 ===
Fig. 5: The structure of the proposed AMN.
Encoded action
Î±
Î²
Î·
000 0000
0
0
0
001 0000
1
0
0
010 0000
0
LEO 1 = 1
LEO 1 = 0
011 0000
0
LEO 1 = 0
LEO 1 = 1
100 0000
0
HAPS 1 = 1
HAPS 1 = 0
110 0000
0
HAPS 1 = 0
HAPS 1 = 1
TABLE I: One example of the proposed encoding scheme.
(probability generation), and propose an efficient encoding
scheme to significantly reduce the associated complexity.
Specifically, the encoded action can be divided into two
parts, namely the basic offloading decision (BOD) part and
the LEO/HAPS index (LHI) part. The BOD part represents
the portion of the offloading decisions: drop the task (000),
local process (001), offload to LEO (010), offload to one
LEO and then another (011), offload to HAPS (100), and
offload to one HAPS and then another (110). The LHI part
represents another portion of the offloading decision: The exact
LEO/HAPS that receives the task from the user. As the total
number of LEO/HAPS may change, the length of the LHI
part changes according to the exact number of LEO/HAPS.
For example, 4 for 16 LEO/HAPSs. In contrast, the length of
the BDO part stays unchanged, which is 3. Table I shows one
example of the mapping between Î±, Î², Î· and the proposed
encoding scheme. Specifically, the first three digits of the
encoded action represent the BOD part, and the rest four digits
indicate the LHI part.
Following the encoding of the discrete actions, a unique list
of available discrete actions for each user is then produced.
Specifically, each user maintains a list of available actions
according to the encoding scheme and the connection status
between user and LEO/HAPS. Consequently, the encoderâ€™s
output is the probability of the action index within each
userâ€™s list. Thereby it is guaranteed that no invalid actions are
generated, which, in turn, reduces the output complexity. It is
clear that our encoding scheme could dramatically lower the
complexity in the probability generation process. The mean
square error (MSE) between the restored latent action and the
input latent action is used to train the AMN. The detailed
training settings will be described in Section V.
C. Practical Deployment
In practice, GDRL can be deployed as a task offloading
and resource allocation service in the cloud. Specifically, it
can be implemented as a virtual machine (VM) instance in
OpenStack. OpenStack components can be used to launch and
manage these GDRL VM instances. Vendors can train multiple
GDRL models for various scenarios and securely store and
access the trained parameters interested in the cloud.
V. THE PROPOSED META-GDRL
A. Motivation
Section IV demonstrates that the proposed GDRL can
efficiently generate task offloading and resource allocation
schemes within a given SAGIN environment. However, practical applications can be complicated by dramatic changes
in key parameters such as available resources, LEO/HAPS
locations, and task-generation-related parameters due to LEO
and aerial platform movements and dynamic user traffic. DRLbased approaches have the advantage of direct applicability
to new scenarios but suffer inference performance degradation
due to model mismatch when training and testing environments
follow different distributions [33].
We address this challenge by integrating the ModelAgnostic Meta-Learning (MAML) approach into our GDRL
framework. MAML, designed for meta-learning, allows the
DRL method to quickly adapt to new environments using a
few auxiliary training data collected from these new settings
[34].
B. Definition
We generate hyperparameters for user request generation
and total initial resources, following the distribution p (E),
where E represents the environmental task. For clarity, â€˜environmental tasksâ€™ refer to environments with different hyperparameters while â€˜taskâ€™ refer to user-generated tasks (T ).

=== PAGE 11 ===
Fig. 6: The learning process of MAML.
MAML is based on the concept that its networkâ€™s internal
representation, denoted as Î¸m, of the distribution p (E), shares
common features across various tasks. This allows rapid adaptation to new environmental tasks emerging from the same
distribution. The learning process of MAML consists of two
loops: an inner loop adapting Î¸m to a specific environmental
task and an outer loop updating the initial model based on
loss from all sampled environmental tasks. Fig. 6 shows the
structure of the learning process. The inner loop calculates Î¸i
for each environmental task from Î¸m without directly altering
Î¸m directly. Each environmental task Ei has a unique set
of adjustable parameters, Î¸i, which can be updated once or
multiple times. Here, we use Ds
i to represent the supporting
dataset for environmental task Ei. Accordingly, the one-step
update for the inner loop is
Î¸i = Î¸m âˆ’Î»inâˆ‡Î¸mLi (Î¸m; Ds
i ) ,
(44)
where Î»in is the learning rate for the inner loop, and Li denotes
the loss function for environmental task Ei. We start with an
initial model f(Î¸m). After updating in the inner loop update,
we assign a unique f(Î¸i) to each Ei. Using these updated
models, we generate a query dataset. The outer loop then
begins by refining the initial model f(Î¸m) based on the loss
obtained from all environmental tasks using the query dataset.
Specifically, the outer loop updates Î¸m by minimizing the loss
min
Î¸
X
Eiâˆ¼p(E)
Li(Î¸i; Dq
i),
(45)
where Dq
i represents the query dataset. Accordingly, the Î¸m is
updated as
Î¸m = Î¸m âˆ’Î»outâˆ‡Î¸m
X
Eiâˆ¼p(E)
Li (Î¸i; Dq
i ) ,
(46)
where Î»out represents the learning rate for outer loop.
C. Implementation
To implement the MAML in TRPO, we need to examine
the loss functions of TRPO and their gradients relative to the
network parameters. The loss function Eq. (35) for the TRPO
policy network measures the performance between the new
police Ï€new and the old one Ï€old. This can be generalized as
follows.
LÏ€old(Ï€new) =
E
o,aâˆ¼Ï€old
Ï€new
Ï€old
AÏ€old(o, a)

,
(47)
where the advantage function AÏ€old(o, a) can be approximated
by the estimated value AGAE(o, a) using the generalized
advantage estimation (GAE) method as follows.
AGAE(o(t), a(t)) =
âˆ
X
l=0
(gn)lÎ´(t + l),
(48)
and
Î´(t + l) = V(o(t)) + gVÏ€(o(t + 1)) âˆ’VÏ€(o(t)),
(49)
where V(o(t)) represents the reward function, n âˆˆ[0, 1]
represents the variance reduction parameter which determines
the trade-off between bias and variance. According to [35], the
loss function of MAML Li can be calculated as
Li = âˆ’LÏ€i
old(Ï€i
new),
(50)
where Ï€i represents the policy obtained in Ei. Therefore,
combining Eq. (44) and Eq. (50), the one-step update of
parameters for inner loop is
Î¸i
new = Î¸m + Î»inâˆ‡Î¸mLÏ€i
old(Ï€i
new).
(51)
Note that the difference between f(Î¸new) and f(Î¸m) should be
within the trust region for a monotonic improvement guarantee.
Accordingly, considering Eq. (39), the final update should be
Î¸i
new = Î¸m + Îº
s
2Î´
zT Câˆ’1
Î¸mzCâˆ’1
Î¸mz,
(52)
where the gradient z and can be calculated based on
z =
1
ST
ST
X
t=0
Ë†A (o(t), a(t)) âˆ‡Î¸i log Ï€ (a(t) | o(t)) .
(53)
The outer loop for the update of Î¸m should be
Î¸m = Î¸m + Î»outâˆ‡Î¸m
X
Eiâˆ¼p(E)
LÏ€i
old(Ï€i
new).
(54)
Note that for the outer loop, we can choose a value of Î»out
and do not need to satisfy the trust region constraint. For the
value network of TRPO, the loss function is
Lpolicy =
ST
X
t=0
(VÏ• (o(t)) âˆ’Î¶(o(t)))2 ,
(55)
and thus the inner loop one-step update for the value network
can be formulated as
Ï•i = Ï•m âˆ’Î»inâˆ‡Ï•mLpolicy,
(56)
and the outer loop for the value network is
Ï•m = Ï•m âˆ’Î»out
X
Eiâˆ¼p(E)
âˆ‡Ï•mLpolicy.
(57)
The implementation of the MAML-TRPO is shown in Algorithm 1.

=== PAGE 12 ===
Algorithm 1 MAML-TRPO
1: Iin: number of inner loop steps
2: Iout: number of outer loop steps
3: E: number of environmental tasks
4: for Iout iterations do
5:
Sample E environmental tasks: Ei âˆ¼p(E)
6:
for each Ei do
7:
Copy the parameter Î¸m to Î¸i of Ei
8:
for Iin iterations do
9:
Interact with the environment
10:
Compute AGAE(o(t), a(t)) using Eq. (48)
11:
Compute z using Eq. (53)
12:
Update Î¸i
new using Eq. (52)
13:
Update Ï•i using Eq. (56)
14:
end for
15:
Calculate Li using Eq. (50)
16:
Calculate Lpolicy using Eq. (55)
17:
Update Î¸m using Eq. (54)
18:
Update Ï•m using Eq. (57)
19:
end for
20: end for
Fig. 7: MSE between latent actions from TRPO and AMN.
VI. SIMULATION RESULTS
In this section, simulation results are provided to validate
the superiority of our proposed GDRL from two perspectives.
We first validate that our proposed GDRL has the best latency
performance compared with other baseline methods to solve
the SAGIN task offloading and resource allocation problem
P. Then, we verify that the proposed GDRL, with its built-in
MAML, is robust against network parameter variation.
The relevant parameters in the simulation are listed as
follows. For the task generation, we have DT su(t)
âˆˆ
[8000, 10000] bits, DS su(t) âˆˆ[200, 500] bits, DT vu(t) âˆˆ
[5000, 6000] bits, DS vu(t) âˆˆ[50, 200] bits, DT and DS
ou(t) âˆˆ[1000, 3000], LEO Cj âˆˆ[100, 130], HAPS Cj âˆˆ
[50, 80], and Ï„ (max) = 106 Hz. For the training of GDRL,
batch size is 128, experience horizon is 1024, activation
function is Sigmoid, and optimizer is Adam. We also have
B = 15 kHz, Ïµd
ÂµD
u
= 10âˆ’3, Tf = 10âˆ’3 (s), K = 100,
N0 = âˆ’173 dBm/Hz, Gl/n
u
= 10, and M l/n
x/y = 6.
Fig. 8: Value function loss for GDRL.
To illustrate the impact of learning rate on AMN reconstruction performance, Fig. 7 shows the mean square error
(MSE) between the latent actions generated by TRPO and
those reconstructed by AMN during training for three different learning rates. Both continuous and discrete scenarios
of AMN are included. It can be clearly seen that learning
rates of 0.01 and 0.001 exhibit significantly lower variance
compared to a learning rate of 0.1, indicating that they achieve
similar but superior reconstruction performance relative to 0.1.
Consequently, we will use the smallest learning rate (0.001) in
training AMN, as it outperforms 0.01 in reducing instability
and overshooting probabilities
To reveal the impact of the number of training loops on value
function loss of GDRL, we have carried out simulations for
three training loops, and the results are presented in Fig. 8.
Here, the value function loss is the difference between the
GDRLâ€™s expectation of a stateâ€™s value and the empirically
observed value of that state. Note that, to collect latent actions
and perform alternative training between AMN and TRPO,
multiple training loops must be conducted. The solid lines
represent the smoothed loss values, while the background near
transparent lines indicate the actual loss at each timestep. It
is clearly shown that, for each iteration, the general trend
is that the smoothed value function loss reduces as learning
progresses. It also reveals that the smoothed value function
loss is lowered with the increase of the loop index, indicating
the convergence of the alternative training between AMN and
TRPO. When it comes to the actual value function loss, it is
observed that, as the training index increases, the loss fluctuation range significantly narrows, indicating the improvement
of the system stability.
Fig.
9 shows the inference task offloading and resource
allocation decisions made by GDRL for one user after training, providing insight into GDRLâ€™s decision-making process. The x-axis is divided into three components: task type
(DS/DT), timestep (1-21), and server (HAPS/LEO/HAPS(i)),
with HAPS(i) indicating that the task was initially offloaded to
one HAPS before being transferred to another. For demonstration purposes, we randomly selected 20 consecutive timesteps.
The figure shows that most DS tasks are offloaded to HAPS
while DT tasks go to LEO due to its proximity and lower
latency compared to LEO. After allocating tasks to HAPS

=== PAGE 13 ===
Fig. 9: The task offloading and resource allocation actions for
different service types.
during several timesteps (2-7), GDRL opts for local processing
of DS tasks at steps 8 and 11 in order to avoid violating
resource constraints. This decision may also be influenced by
frequency resource allocation as seen in previous timestep (67) where a large portion of frequency was allocated.
A. Baseline Methods for Comparison
Here, the baseline methods adopted for comparison are
introduced.
1) Random: All offloading and resource allocation decisions
are randomly generated. It is clear that this method has
the worst performance.
2) TRPO [36]: TRPO is an on-policy DRL method where
the policy network directly generates actions. TRPO has
a monotonic improvement guarantee compared with other
methods.
3) PPO [37]: As another on-policy DRL method, proximal
policy optimization (PPO) can be seen as a simplified
version of TRPO with a much lower computational complexity.
4) SAC [38]: As an off-policy DRL method, SAC is featured
by regularization. Specifically, SAC is trained to balance
between return and entropy, which resembles the trade-off
between exploration and exploitation.
5) DDPG [39]: DDPG is also an off-policy DRL method.
Closely related to Q-learning, it learns a Q-function by
utilizing off-policy data and the Bellman equation, based
on which a policy is learned.
Our DRL framework requires both discrete and continuous
actions. However, standard baselines such as TRPO, DDPG,
SAC, and PPO, can only generate either discrete or continuous
actions. To enable them to output both types of actions, we
incorporate AMN into their architectures to perform performance comparisons.
B. Performance Comparisons
Fig. 10 compares the return between our proposed GDRL
and the baseline methods. It can be seen that GDRL achieves
the highest return among all methods. For off-policy methods
DDPG and SAC, each of them converges to its own suboptimal solution which is worse than on-policy methods TRPO
Fig. 10: Return comparison for GDRLs and the baseline
methods.
Fig. 11: The inference cumulative latency comparison between
proposed GDRL and baseline methods.
and PPO. Note that, compared with on-policy methods, offpolicy methods introduce more randomness into the environment and thus are less robust in solving problem P. It is
also seen that for GDRL, the case with GFEN significantly
outperforms the case without GFEN. This clearly verifies the
benefit that GFEN brings to GDRL. It is also observed that the
initial training result for our proposed GDRL with GFEN is
significantly higher than all other methods. This is because,
as stated in Section VI A, GDRL has been trained twice
beforehand, while all the others have not been trained at all
beforehand. Overall, Fig. 10 substantiates the superiority of
our proposed GDRL over the baseline methods.
After training, a comparison is carried out in Fig. 11 in terms
of cumulative latency. It can be seen that GDRL achieves the
lowest cumulative latency consistently across all timesteps. In
contrast, as expected, the random method is inferior to all the
other methods. It can also be seen that all methods exhibit an
increase in cumulative latency over time. This trend is to be
expected, as there is an inherent accumulation of processed
tasks over time. Furthermore, GDRL has the least slope,
indicating that it has the lowest rate of latency accumulation as
time progresses. This verifies that GDRL can efficiently min-

=== PAGE 14 ===
Fig. 12: Inference latency comparison for different LEO/HAPS
available resources scenarios.
imize latency and maintain long-term consistent performance.
It can also be noticed that the slopes of DDPG and SAC are
steeper compared to TRPO and PPO, which is consistent with
other results. In summary, GDRLâ€™s performance in latency
management outperforms the baseline methods, evidencing its
superior efficiency in solving P.
Note that, in Fig. 12, the total available resource is assumed
to be fixed. To evaluate these methods in a more practical
environment, we compare the inference latency in Fig. 12
for three scenarios where the total available resources of the
LEO/HAPS vary with time. For Scenarios 1-3, LEO and
HAPS resources are set to be between 150-200 and 100150, between 100-150 and 50-80, and between 50-80 and
30-60, respectively. Specifically, Scenarios 1-3, in ascending
order, represent abundant, moderate, and stringent resource
availability, respectively. For each of these five methods, it
is shown in Fig.
12 that, consistent with expectation, the
inference latency increases with the reduction in available
resource levels. In addition, in all three scenarios, GDRL
achieves significantly lower inference latency than all the other
four methods, verifying the advantage of GDRL regardless of
the available resource level. It is also observed that, in all
resource levels, on-policy methods TRPO and PPO result in
lower latency than off-policy methods SAC and DDPG, which
is consistent with the observation in Figs. 10 and 11. Overall,
Fig. 12 clearly proves that GDRL performs best in resource
allocation to each task to reduce latency for a broad range of
available resource levels.
A similar comparison is carried out in Fig. 13 for two
different task generation frequencies, where Ïµ=0.9 embodies a
high task generation probability scenario and Ïµ=0.7 a moderate
one. It can be seen that, for each method, latency increases
when Ïµ increases from 0.7 to 0.9, as fewer tasks are generated
for Ïµ=0.7. In addition, the relative supremacy of these methods
in terms of latency is the same as that in Fig. 12.
To verify the robustness of GDRL with MAML against
variation of the critical parameters of SAGIN, we present two
radar charts in Fig. 14 comparing the performance of GDRL
and the four baseline methods for three different environmental
tasks randomly sampled from the distribution p(E). In the
figure, a point further away from the center of the hexagons
Fig. 13: Inference latency comparison under different task
generation rate Ïµ.
indicates a higher reward (Fig. 14 (a)) or latency (Fig. 14
(b)). It can be seen in Fig. 14 (a) that GDRL without MAML
performs better than the four baseline methods for all three
environmental tasks, which clearly verifies the benefits of
GFEN. Furthermore, compared to GDRL without MAML,
GDRL with MAML achieves significantly higher rewards.
When it comes to latency, it can be seen in Fig. 14 (b) that the
relative supremacy between these six methods are the same as
that in Fig. 14 (a).
In summary, all simulation results presented in this section
validate the convergence of GDRL, the best performance of
GDRL in solving problem P, the performance boost of implementing GFEN and AMN, and the robustness enhancement
provided by MAML.
C. Computational Complexity and Scalability
The computational complexity of DL-based methods consists of two components: training complexity and inference
complexity. The training complexity mainly originates from
parameter update and backpropagation, and the inference complexity originates from generating actions when parameters
stay unchanged.
1) Training Complexity: The training complexity can be
divided into three parts: The GFEN, the LAGN, and the AMN.
The GFEN consists of two layers of GCN and four layers of
multi-layer perception (MLP). The computational complexity
can be represented as [40]
CMLP = nin1 +
4
X
l=2
nlâˆ’1nl + n4no,
CGCN =
X
i
F 2
i |E| + Fi|V|,
CGFEN = CGCN + CMLP,
(58)
for 4-layer MLP, 2-layer GCN, and GFEN, respectively. Here,
ni represents the input size, n1 and n2 denote the number of
neurons in the first and second layers of the MLP, respectively,
Fi represents the node features for the i-th layer of GCN, |E|
represents the number of edges, and |V| represents the number
of nodes.

=== PAGE 15 ===
Fig. 14: The performance comparisons between GDRL methods and baseline methods.
The LAGN is constructed by a four-layer MLP critic network and a four-layer MLP actor network. The computational
complexity can be calculated as [41]
CLAGN = 2

nin1 +
 3
X
l=1
nlnl+1
!
+ n3no
!
.
(59)
The AMN consists of four layers of MLP, whose complexity
for each layer is given in Eq. (58), and two layers of LSTM,
whose complexity for each layer can be calculated as
CLSTM = ninh(4ni + 4nh + 3 + no),
(60)
where nh is the number of hidden units. Therefore, the
computational complexity of AMN can be calculated as
CAMN = 2CLSTM + CMLP.
(61)
2) Inference Complexity: As the GDLR model is trained
offline, the online inference complexity remains very low,
which can be expressed as O(nall) [42], where nall denotes
the overall number of neurons.
3) Complexity Comparison: The computational complexity
comparison can only be carried out when schemes in [22],
[23] and the baseline methods (TRPO, SAC, PPO, DDPG)
also incorporate AMN. This is because, without AMN, the
discrete action space size is 2+2(N+L)+2(N+L)2, which grows
exponentially with the number of LEOs/HAPSs and prevents
these schemes from converging. By incorporating AMN, this
size is reduced to 23+log2(max(N,L)), making it manageable.
When all other schemes include AMN, GDRL exhibits a
higher training complexity due to GFEN implementation.
However, since only two node features are considered here,
GFENâ€™s extra complexity is low and increases linearly with
the number of neurons. Simulations validate that this small
increase in complexity significantly enhances performance.
The main factor that influences the system scalability is
computational complexity. GDRL incorporates the AMN and
encoding scheme which significantly reduces offline training
complexity. Furthermore, GDRL can be easily deployed online
with very low post-training complexity. Therefore, GDRL
offers high scalability and can support a large number of UEs,
LEOs, and HAPS for practical applications.
4) Scalability: The main factor that influences the system
scalability is computational complexity. GDRL incorporates
AMN and encoding scheme which significantly reduces the
computational complexity for offline training. Furthermore,
GDRL, a DRL-based method, can be easily deployed online
with very low complexity after training. Therefore, GDRL
offers high scalability and can support a large number of UEs,
LEOs, and HAPS for practical applications.
VII. CONCLUSION
In this paper, we aimed to solve the task offloading and
resource allocation problem in SAGIN that mirrors real-world
conditions. Specifically, we considered a SAGIN model that
encompasses different services and distinct features of each
segment. To have a high-performance real-time low complexity
task offloading and resource allocation scheme, we proposed
GDRL to capture graphical features and directly generate
actions by interacting with the environment. In detail, we
designed GFEN to extract features from graph-structured states
efficiently and AMN with the encoding scheme to enable endto-end generation of both discrete offloading and continuous
resource allocation actions. Furthermore, we proposed the
integration of MAML in GDRL to enable the fast adaptation of
GDRL to the critical parameter changes of SAGIN environment, which enabled the low-complexity online deployment
of GDRL. Simulation results validate that GDRL significantly
outperforms state-of-the-art methods in terms of latency performance. In our future work, we will consider the correlation
between tasks and transform the centralized decision-making
framework into a distributed one.
REFERENCES
[1] J. Liu, Y. Shi, Z. M. Fadlullah, and N. Kato, â€œSpace-Air-Ground Integrated Network: A Survey,â€ IEEE Communications Surveys & Tutorials,
vol. 20, no. 4, pp. 2714â€“2741, Mar 2018.
[2] F. Rinaldi, H.-L. Maattanen, J. Torsner, S. Pizzi, S. Andreev, A. Iera,
Y. Koucheryavy, and G. Araniti, â€œNon-Terrestrial Networks in 5G &
Beyond: A Survey,â€ IEEE Access, vol. 8, pp. 165 178â€“165 200, Sep 2020.
[3] R. Xie, Q. Tang, Q. Wang, X. Liu, F. R. Yu, and T. Huang, â€œSatelliteTerrestrial Integrated Edge Computing Networks: Architecture, Challenges, and Open Issues,â€ IEEE Network, vol. 34, no. 3, pp. 224â€“231,
Mar 2020.
[4] Z. Zhang, Y. Li, C. Huang, Q. Guo, L. Liu, C. Yuen, and Y. L.
Guan, â€œUser Activity Detection and Channel Estimation for Grant-Free
Random Access in LEO Satellite-Enabled Internet of Things,â€ IEEE
Internet of Things Journal, vol. 7, no. 9, pp. 8811â€“8825, May 2020.

=== PAGE 16 ===
[5] Y. Mao, C. You, J. Zhang, K. Huang, and K. B. Letaief, â€œA Survey
on Mobile Edge Computing: The Communication Perspective,â€ IEEE
Communications Surveys & Tutorials, vol. 19, no. 4, pp. 2322â€“2358,
Aug 2017.
[6] D. C. Nguyen, P. Cheng, M. Ding, D. Lopez-Perez, P. N. Pathirana, J. Li,
A. Seneviratne, Y. Li, and H. V. Poor, â€œEnabling AI in Future Wireless
Networks: A Data Life Cycle Perspective,â€ IEEE Communications
Surveys & Tutorials, vol. 23, no. 1, pp. 553â€“595, Sep 2021.
[7] P. Cheng, Y. Chen, M. Ding, Z. Chen, S. Liu, and Y.-P. P. Chen,
â€œDeep Reinforcement Learning for Online Resource Allocation in IoT
Networks: Technology, Development, and Future Challenges,â€ IEEE
Communications Magazine, vol. 61, no. 6, pp. 111â€“117, Jun 2023.
[8] Q. Chen, W. Meng, T. Q. S. Quek, and S. Chen, â€œMulti-Tier Hybrid
Offloading for Computation-Aware IoT Applications in Civil AircraftAugmented SAGIN,â€ IEEE Journal on Selected Areas in Communications, vol. 41, no. 2, pp. 399â€“417, Dec 2023.
[9] P. Mach and Z. Becvar, â€œMobile Edge Computing: A Survey on Architecture and Computation Offloading,â€ IEEE Communications Surveys &
Tutorials, vol. 19, no. 3, pp. 1628â€“1656, Mar 2017.
[10] M. D. Nguyen, L. B. Le, and A. Girard, â€œJoint Computation Offloading,
UAV Trajectory, User Scheduling, and Resource Allocation in SAGIN,â€
in GLOBECOM 2022 - 2022 IEEE Global Communications Conference,
Dec 2022, pp. 5099â€“5104.
[11] Y. Cai, P. Cheng, Z. Chen, M. Ding, B. Vucetic, and Y. Li, â€œDeep
reinforcement learning for online resource allocation in network slicing,â€
IEEE Transactions on Mobile Computing, vol. 23, no. 6, pp. 7099â€“7116,
Oct 2024.
[12] P. Qin, M. Wang, X. Zhao, and S. Geng, â€œContent Service Oriented
Resource Allocation for Spaceâ€“Airâ€“Ground Integrated 6G Networks:
A Three-Sided Cyclic Matching Approach,â€ IEEE Internet of Things
Journal, vol. 10, no. 1, pp. 828â€“839, Sep 2023.
[13] A. Asheralieva, D. Niyato, and X. Wei, â€œUltrareliable Low-Latency
Slicing in Spaceâ€“Airâ€“Ground Multiaccess Edge Computing Networks
for Next-Generation Internet of Things and Mobile Applications,â€ IEEE
Internet of Things Journal, vol. 11, no. 3, pp. 3956â€“3978, Jul 2024.
[14] P. Zhang, Y. Li, N. Kumar, N. Chen, C.-H. Hsu, and A. Barnawi,
â€œDistributed Deep Reinforcement Learning Assisted Resource Allocation
Algorithm for Space-Air-Ground Integrated Networks,â€ IEEE Transactions on Network and Service Management, vol. 20, no. 3, pp. 3348â€“
3358, Dec 2023.
[15] Y. Liu, L. Jiang, Q. Qi, K. Xie, and S. Xie, â€œOnline Computation Offloading for Collaborative Space/Aerial-Aided Edge Computing Toward
6G System,â€ IEEE Transactions on Vehicular Technology, vol. 73, no. 2,
pp. 2495â€“2505, Sep 2024.
[16] X. Zhang, J. Liu, R. Zhang, Y. Huang, J. Tong, N. Xin, L. Liu, and
Z. Xiong, â€œEnergy-Efficient Computation Peer Offloading in Satellite
Edge Computing Networks,â€ IEEE Transactions on Mobile Computing,
pp. 1â€“15, Apr 2023.
[17] Z. Song, Y. Hao, Y. Liu, and X. Sun, â€œEnergy-Efficient Multiaccess Edge
Computing for Terrestrial-Satellite Internet of Things,â€ IEEE Internet of
Things Journal, vol. 8, no. 18, pp. 14 202â€“14 218, Mar 2021.
[18] X. Cao, B. Yang, Y. Shen, C. Yuen, Y. Zhang, Z. Han, H. V. Poor, and
L. Hanzo, â€œEdge-Assisted Multi-Layer Offloading Optimization of LEO
Satellite-Terrestrial Integrated Networks,â€ IEEE Journal on Selected
Areas in Communications, vol. 41, no. 2, pp. 381â€“398, Dec 2023.
[19] Y. K. Tun, K. T. Kim, L. Zou, Z. Han, G. DÂ´an, and C. S. Hong,
â€œCollaborative Computing Services at Ground, Air, and Space: An
Optimization Approach,â€ IEEE Transactions on Vehicular Technology,
vol. 73, no. 1, pp. 1491â€“1496, Aug 2024.
[20] Y. K. Tun, Y. M. Park, N. H. Tran, W. Saad, S. R. Pandey, and C. S.
Hong, â€œEnergy-Efficient Resource Management in UAV-Assisted Mobile
Edge Computing,â€ IEEE Communications Letters, vol. 25, no. 1, pp.
249â€“253, Sep 2021.
[21] S. Mao, S. He, and J. Wu, â€œJoint UAV Position Optimization and
Resource Scheduling in Space-Air-Ground Integrated Networks With
Mixed Cloud-Edge Computing,â€ IEEE Systems Journal, vol. 15, no. 3,
pp. 3992â€“4002, Dec 2021.
[22] T. K. Rodrigues and N. Kato, â€œHybrid Centralized and Distributed
Learning for MEC-Equipped Satellite 6G Networks,â€ IEEE Journal on
Selected Areas in Communications, vol. 41, no. 4, pp. 1201â€“1211, Feb
2023.
[23] N. Cheng, F. Lyu, W. Quan, C. Zhou, H. He, W. Shi, and X. Shen,
â€œSpace/Aerial-Assisted Computing Offloading for IoT Applications: A
Learning-Based Approach,â€ IEEE Journal on Selected Areas in Communications, vol. 37, no. 5, pp. 1117â€“1129, Mar 2019.
[24] F. Chai, Q. Zhang, H. Yao, X. Xin, R. Gao, and M. Guizani, â€œJoint MultiTask Offloading and Resource Allocation for Mobile Edge Computing
Systems in Satellite IoT,â€ IEEE Transactions on Vehicular Technology,
vol. 72, no. 6, pp. 7783â€“7795, Jan 2023.
[25] S. S. Hassan, Y. M. Park, Y. K. Tun, W. Saad, Z. Han, and C. S. Hong,
â€œSatellite-Based ITS Data Offloading & Computation in 6G Networks:
A Cooperative Multi-Agent Proximal Policy Optimization DRL With
Attention Approach,â€ IEEE Transactions on Mobile Computing, pp. 1â€“
18, Aug 2023.
[26] H. Shen, Y. Tian, T. Wang, and G. Bai, â€œSlicing-Based Task Offloading
in Space-Air-Ground Integrated Vehicular Networks,â€ IEEE Transactions
on Mobile Computing, pp. 1â€“15, Jun 2023.
[27] J. Zhou, G. Cui, S. Hu, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li, and
M. Sun, â€œGraph neural networks: A review of methods and applications,â€
AI open, vol. 1, pp. 57â€“81, Jan 2020.
[28] C. She, C. Yang, and T. Q. S. Quek, â€œCross-Layer Optimization for UltraReliable and Low-Latency Radio Access Networks,â€ IEEE Transactions
on Wireless Communications, vol. 17, no. 1, pp. 127â€“141, Oct 2018.
[29] N. Cesa-Bianchi and G. Lugosi, Prediction, learning, and games. Cambridge university press, 2006.
[30] J. Nievergelt, â€œExhaustive search, combinatorial optimization and enumeration: Exploring the potential of raw computing power,â€ in International Conference on Current Trends in Theory and Practice of Computer
Science.
Springer, 2000, pp. 18â€“35.
[31] T. N. Kipf and M. Welling, â€œSemi-supervised classification with graph
convolutional networks,â€ arXiv preprint arXiv:1609.02907, Sep 2016.
[32] D. K. Hammond, P. Vandergheynst, and R. Gribonval, â€œWavelets on
graphs via spectral graph theory,â€ Applied and Computational Harmonic
Analysis, vol. 30, no. 2, pp. 129â€“150, Dec 2011.
[33] Y. Hu, X. Wang, and W. Saad, â€œDistributed and Distribution-Robust Meta
Reinforcement Learning (D2-RMRL) for Data Pre-Storage and Routing
in Cube Satellite Networks,â€ IEEE Journal of Selected Topics in Signal
Processing, vol. 17, no. 1, pp. 128â€“141, Jan 2023.
[34] G. Qu, H. Wu, R. Li, and P. Jiao, â€œDMRO: A Deep Meta Reinforcement
Learning-Based Task Offloading Framework for Edge-Cloud Computing,â€ IEEE Transactions on Network and Service Management, vol. 18,
no. 3, pp. 3448â€“3459, Jun 2021.
[35] Z. Lu and M. C. Gursoy, â€œDynamic channel access via metareinforcement learning,â€ in 2021 IEEE Global Communications Conference (GLOBECOM).
IEEE, 2021, pp. 01â€“06.
[36] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, â€œTrust
region policy optimization,â€ in International conference on machine
learning.
PMLR, 2015, pp. 1889â€“1897.
[37] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, â€œProximal policy optimization algorithms,â€ arXiv preprint arXiv:1707.06347,
Jul 2017.
[38] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, â€œSoft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic
actor,â€ in International conference on machine learning.
PMLR, Aug
2018, pp. 1861â€“1870.
[39] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
D. Silver, and D. Wierstra, â€œContinuous control with deep reinforcement
learning,â€ arXiv preprint arXiv:1509.02971, Sep 2015.
[40] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip, â€œA
comprehensive survey on graph neural networks,â€ IEEE transactions on
neural networks and learning systems, vol. 32, no. 1, pp. 4â€“24, Jan 2020.
[41] P. J. Freire, Y. Osadchuk, B. Spinnler, A. Napoli, W. Schairer, N. Costa,
J. E. Prilepsky, and S. K. Turitsyn, â€œPerformance versus complexity
study of neural network equalizers in coherent optical systems,â€ Journal
of Lightwave Technology, vol. 39, no. 19, pp. 6085â€“6096, Jul 2021.
[42] Y. Xu, Z. Zhao, P. Cheng, Z. Chen, M. Ding, B. Vucetic, and Y. Li,
â€œConstrained Reinforcement Learning for Resource Allocation in Network Slicing,â€ IEEE Communications Letters, vol. 25, no. 5, pp. 1554â€“
1558, Jan 2021.

=== PAGE 17 ===
Yue Cai received her B.E. in telecommunication
engineering from Beijing Jiaotong University, China,
and her M.Phil in Electrical and Information Engineering from the University of Sydney, Australia.
She is currently working towards his Ph.D. degree in
Electrical and Information Engineering at The University of Sydney, Australia. Her research interests
include wireless AI, machine learning, MIMO, IoT,
and satellites.
Peng Cheng (Mâ€™12) received the B.S. and M.S
degrees with great honors in communication and information systems from University of Electronic Science and Technology of China (UESTC), Chengdu,
China, in 2006 and 2009 and the Ph.D. degree from
Shanghai Jiao Tong University, Shanghai, China, in
2013. From 2014 to 2017, he was a Postdoctoral
Research Scientist in CSIRO, Sydney, Australia.
From 2017 to 2020, he was an ARC DECRA Fellow/Lecturer at the University of Sydney, Australia.
He is currently an ARC DECRA Fellow and a
Senior Lecturer (Tenured Associate Professor in U.S. systems) in Department
of Computer Science and Information Technology, La Trobe University,
Australia, and is affiliated with the University of Sydney, Australia. He has
published over 100 peer-reviewed research papers in leading international
journals and conferences. His current research interests include wireless AI,
machine learning, IoT, millimeter-wave communications, and compressive
sensing theory.
Zhuo Chen (Sâ€™01-Mâ€™05-SMâ€™18) received the B.S.
degree in electrical engineering from Shanghai Jiao
Tong University, Shanghai, China, in 1997 and the
M.S. and Ph.D. degrees from the School of Electrical and Information Engineering, the University
of Sydney, Sydney, Australia, in 2001 and 2004,
respectively. He was a Senior Research Scientist
with Commonwealth Scientific and Industrial Research Organization (CSIRO), Sydney, Australia. His
research interests include wireless communications,
machine learning, and wireless sensor networks.
Wei Xiang (Sâ€™00â€“Mâ€™04â€“SMâ€™10) is the Cisco Research Chair of AI and IoT and Director of the CiscoLa Trobe Centre for AI and IoT at La Trobe University. Previously, he was Foundation Chair and Head
of Discipline of IoT Engineering at James Cook University. His leadership in establishing Australiaâ€™s first
accredited IoT Engineering degree program earned
him induction into the Pearcy Foundationâ€™s Hall of
Fame in 2018. He is an elected Fellow of the IET
(UK) and Engineers Australia. He received the TNQ
Innovation Award (2016), Pearcey Entrepreneurship
Award (2017), and Engineers Australia Cairns Engineer of the Year (2017).
He co-received four Best Paper Awards at WiSATSâ€™2019, WCSPâ€™2015, IEEE
WCNCâ€™2011, and ICWMCâ€™2009. His research interests include the Internet
of Things, wireless communications, machine learning for IoT data analytics,
and computer vision.
Branka Vucetic (Fâ€™03) is an ARC Laureate Fellow
and Director of the Centre of Excellence for IoT
and Telecommunications at the University of Sydney.
Her current research work is in wireless networks
and the Internet of Things. In the area of wireless
networks, she works on communication system design for millimeter wave frequency bands. In the
area of the Internet of Things, Vucetic works on
providing wireless connectivity for mission critical
applications. Branka Vucetic is a Fellow of IEEE,
the Australian Academy of Technological Sciences
and Engineering and the Australian Academy of Science.
Yonghui Li (Mâ€™04-SMâ€™09-Fâ€™19) received his PhD
degree in November 2002 from Beijing University
of Aeronautics and Astronautics. From 1999- 2003,
he was affiliated with Linkair Communication Inc,
where he held a position of project manager with
responsibility for the design of physical layer solutions for the LAS-CDMA system. Since 2003, he
has been with the Centre of Excellence in Telecommunications, the University of Sydney, Australia.
He is now a Professor in School of Electrical and
Information Engineering, University of Sydney. He
is the recipient of the Australian Queen Elizabeth II Fellowship in 2008 and
the Australian Future Fellowship in 2012.
His current research interests are in the area of wireless communications,
with a particular focus on MIMO, millimeter wave communications, machine
to machine communications, coding techniques and cooperative communications. He holds a number of patents granted and pending in these fields. He is
now an editor for IEEE transactions on communications and IEEE transactions
on vehicular technology. He was also the guest editor for IEEE JSAC Special
issue on Millimeter Wave Communications for Future Mobile Networks.
He received the best paper awards from IEEE International Conference on
Communications (ICC) 2014, IEEE PIMRC 2017, and IEEE Wireless Days
Conferences (WD) 2014.
View publication stats