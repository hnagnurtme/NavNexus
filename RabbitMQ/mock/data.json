{
  "Id": "domain-sagin001",
  "Type": "domain",
  "Name": "Space-Air-Ground Integrated Networks (SAGIN)",
  "Synthesis": "Space-Air-Ground Integrated Networks represent a comprehensive integration of satellite networks, aerial platforms (UAVs, HAPs), and terrestrial infrastructure to provide ubiquitous connectivity. Recent research focuses on AI-driven optimization techniques and energy-aware resource management to address the unique challenges of heterogeneous network architectures, dynamic topologies, and varying QoS requirements across different network layers.",
  "WorkspaceId": "Reinforcement-Learning",
  "Level": 0,
  "SourceCount": 2,
  "TotalConfidence": 0.92,
  "CreatedAt": "2025-01-15T08:30:00Z",
  "UpdatedAt": "2025-01-15T14:22:00Z",
  "Children": [
    {
      "Id": "category-optimization001",
      "Type": "category",
      "Name": "AI-Based Network Optimization",
      "Synthesis": "Artificial Intelligence techniques, particularly Deep Reinforcement Learning (DRL) and Multi-Agent Reinforcement Learning (MARL), have emerged as powerful tools for optimizing SAGIN performance. These methods address complex decision-making problems including resource allocation, traffic routing, handover management, and power control in highly dynamic network environments with non-stationary characteristics.",
      "WorkspaceId": "Reinforcement-Learning",
      "Level": 1,
      "SourceCount": 2,
      "TotalConfidence": 0.94,
      "CreatedAt": "2025-01-15T08:35:00Z",
      "UpdatedAt": "2025-01-15T14:22:00Z",
      "Children": [
        {
          "Id": "concept-drl001",
          "Type": "concept",
          "Name": "Deep Reinforcement Learning for Resource Allocation",
          "Synthesis": "DRL algorithms leverage deep neural networks to approximate optimal policies for resource allocation in SAGIN. The English paper demonstrates DQN-based approaches for bandwidth allocation across satellite-aerial-terrestrial links, while the Korean paper extends this with Dueling DQN for energy-efficient scheduling. Both approaches show 15-23% improvement in network throughput compared to traditional heuristic methods.",
          "WorkspaceId": "Reinforcement-Learning",
          "Level": 2,
          "SourceCount": 2,
          "TotalConfidence": 0.95,
          "CreatedAt": "2025-01-15T09:00:00Z",
          "UpdatedAt": "2025-01-15T14:22:00Z",
          "Children": [
            {
              "Id": "subconcept-dqn001",
              "Type": "subconcept",
              "Name": "DQN Architecture for Bandwidth Allocation",
              "Synthesis": "The DQN architecture employs a 4-layer fully-connected network with ReLU activations to map network state observations (link quality, buffer occupancy, user demands) to Q-values for discrete bandwidth allocation actions. Experience replay with prioritized sampling and target network updates every 1000 steps ensure stable learning. The state space includes 47 dimensions covering satellite visibility windows, UAV positions, and ground station loads.",
              "WorkspaceId": "Reinforcement-Learning",
              "Level": 3,
              "SourceCount": 1,
              "TotalConfidence": 0.93,
              "CreatedAt": "2025-01-15T09:15:00Z",
              "UpdatedAt": "2025-01-15T09:15:00Z",
              "Children": [],
              "Evidences": [
                {
                  "Id": "evidence-dqn-en001",
                  "SourceId": "pdf-optimizing-sagin-ai",
                  "SourceName": "Optimizing Space-Air-Ground Integrated Networks by Artificial Intelligence",
                  "ChunkId": "chunk-042",
                  "Text": "We design a DQN agent with a neural network consisting of four fully-connected layers (sizes: 47-256-128-64-32) with ReLU activation functions. The state vector s_t encompasses 47 features including: (i) satellite link signal-to-noise ratios (SNR) for all 6 LEO satellites in view, (ii) UAV positions and remaining battery levels for 8 aerial nodes, (iii) ground station buffer occupancy rates, (iv) current bandwidth allocations across 24 frequency bands, and (v) user traffic demands categorized by priority (emergency, real-time, best-effort). The action space consists of 32 discrete bandwidth allocation policies, each representing a different distribution strategy across the three-tier network. We employ prioritized experience replay with α=0.6 and β=0.4, maintaining a replay buffer of 100,000 transitions. The target network is updated every τ=1000 training steps to stabilize learning.",
                  "Page": 7,
                  "Confidence": 0.93,
                  "CreatedAt": "2025-01-15T09:15:00Z",
                  "Language": "ENG",
                  "SourceLanguage": "ENG",
                  "HierarchyPath": "SAGIN > AI Optimization > DRL > DQN Architecture",
                  "Concepts": ["Deep Q-Network", "State Representation", "Neural Network Architecture", "Experience Replay"],
                  "KeyClaims": [
                    "47-dimensional state space captures comprehensive network conditions",
                    "32 discrete action space enables flexible bandwidth distribution",
                    "Prioritized experience replay improves sample efficiency by 40%",
                    "Target network updates every 1000 steps ensure convergence stability"
                  ],
                  "QuestionsRaised": [
                    "How does state dimensionality impact learning speed and final performance?",
                    "What is the optimal replay buffer size for non-stationary SAGIN environments?"
                  ],
                  "EvidenceStrength": 0.91,
                  "StartPos": 18420,
                  "EndPos": 19587,
                  "ChunkIndex": 42,
                  "HasMore": true,
                  "OverlapLength": 150
                }
              ],
              "GapSuggestions": [
                {
                  "Id": "gap-dqn-rainbow001",
                  "SuggestionText": "Consider investigating Rainbow DQN extensions (combining Double DQN, Dueling DQN, Noisy Nets, and Distributional RL) for improved performance in highly stochastic satellite link conditions",
                  "TargetNodeId": "https://arxiv.org/abs/1710.02298",
                  "TargetFileId": "",
                  "SimilarityScore": 0.87
                },
                {
                  "Id": "gap-dqn-continuous001",
                  "SuggestionText": "Explore continuous action space formulations using DDPG or TD3 for finer-grained bandwidth control instead of discrete allocation policies",
                  "TargetNodeId": "https://arxiv.org/abs/1802.09477",
                  "TargetFileId": "",
                  "SimilarityScore": 0.82
                }
              ]
            },
            {
              "Id": "subconcept-dueling001",
              "Type": "subconcept",
              "Name": "Dueling DQN for Energy-Aware Scheduling",
              "Synthesis": "The Korean paper introduces a Dueling DQN variant that separates value and advantage functions for energy-aware image delivery scheduling. This architecture proves particularly effective for SAGIN scenarios where some state features affect all actions equally (satellite energy budget) while others have action-specific impacts (transmission timing decisions). Results show 18% reduction in energy consumption while maintaining 95% delivery success rate.",
              "WorkspaceId": "Reinforcement-Learning",
              "Level": 3,
              "SourceCount": 1,
              "TotalConfidence": 0.90,
              "CreatedAt": "2025-01-15T09:30:00Z",
              "UpdatedAt": "2025-01-15T09:30:00Z",
              "Children": [],
              "Evidences": [
                {
                  "Id": "evidence-dueling-kr001",
                  "SourceId": "pdf-energy-aware-sagin",
                  "SourceName": "Energy Aware Scheduling and Optimization for Image Delivery in Space-Air-Ground Integrated Networks",
                  "ChunkId": "chunk-038",
                  "Text": "우리는 에너지 인식 이미지 전송을 위해 Dueling DQN 구조를 제안한다. 네트워크는 두 개의 분리된 스트림으로 구성된다: (1) 상태 가치 함수 V(s)를 추정하는 Value Stream (레이어 크기: 52-256-128-1), (2) 각 행동의 이점 A(s,a)를 추정하는 Advantage Stream (레이어 크기: 52-256-128-16). 최종 Q-값은 Q(s,a) = V(s) + (A(s,a) - mean(A(s,·)))로 결합된다. 상태 벡터는 52개 특징을 포함한다: 위성 잔여 에너지 (6개 LEO 위성), UAV 배터리 수준 (10개), 지상국 큐 길이, 이미지 우선순위 레벨 (긴급/일반/저우선순위), 현재 채널 상태 정보 (CSI), 예상 전송 지연. 행동 공간은 16개의 이산 스케줄링 정책으로 구성되며, 각각은 전송 타이밍, 경로 선택, 전력 레벨 조합을 나타낸다. 실험 결과, Dueling 구조는 표준 DQN 대비 18% 에너지 소비 감소와 12% 빠른 수렴을 달성했다.",
                  "Page": 6,
                  "Confidence": 0.90,
                  "CreatedAt": "2025-01-15T09:30:00Z",
                  "Language": "ENG",
                  "SourceLanguage": "KOR",
                  "HierarchyPath": "SAGIN > AI Optimization > DRL > Dueling DQN",
                  "Concepts": ["Dueling DQN", "Energy-Aware Scheduling", "Value-Advantage Decomposition", "Image Delivery"],
                  "KeyClaims": [
                    "Dueling architecture separates state value from action advantages",
                    "52-dimensional state includes energy metrics across all network tiers",
                    "16 discrete actions combine timing, routing, and power decisions",
                    "18% energy reduction with 95% delivery success rate maintained",
                    "12% faster convergence compared to standard DQN"
                  ],
                  "QuestionsRaised": [
                    "How does the value-advantage decomposition help in energy-constrained scenarios?",
                    "What is the trade-off between energy savings and delivery latency?"
                  ],
                  "EvidenceStrength": 0.89,
                  "StartPos": 16240,
                  "EndPos": 17418,
                  "ChunkIndex": 38,
                  "HasMore": true,
                  "OverlapLength": 200
                }
              ],
              "GapSuggestions": [
                {
                  "Id": "gap-dueling-multiobj001",
                  "SuggestionText": "Investigate multi-objective optimization formulations to explicitly balance energy consumption, latency, and throughput using Pareto-based approaches",
                  "TargetNodeId": "https://ieeexplore.ieee.org/document/9234567",
                  "TargetFileId": "",
                  "SimilarityScore": 0.84
                }
              ]
            }
          ],
          "Evidences": [
            {
              "Id": "evidence-drl-overview-en001",
              "SourceId": "pdf-optimizing-sagin-ai",
              "SourceName": "Optimizing Space-Air-Ground Integrated Networks by Artificial Intelligence",
              "ChunkId": "chunk-028",
              "Text": "Deep Reinforcement Learning offers a promising paradigm for autonomous resource management in SAGIN. Unlike traditional optimization methods that require explicit mathematical models of network dynamics, DRL agents learn optimal policies through interaction with the environment. We formulate the resource allocation problem as a Markov Decision Process (MDP) where states represent network conditions (link qualities, traffic loads, energy levels), actions correspond to resource allocation decisions (bandwidth distribution, power control, routing paths), and rewards capture key performance metrics (throughput, energy efficiency, fairness). Our experimental testbed includes 6 LEO satellites in sun-synchronous orbit (altitude: 550 km), 8 UAVs operating at 2-5 km altitude, and 12 ground stations distributed across a 500 km² area. Simulation results demonstrate that DQN-based allocation achieves 23% higher average throughput and 31% lower packet loss rate compared to weighted round-robin baselines under dynamic traffic patterns with temporal variations (peak-to-average ratio: 4.2).",
              "Page": 5,
              "Confidence": 0.95,
              "CreatedAt": "2025-01-15T09:00:00Z",
              "Language": "ENG",
              "SourceLanguage": "ENG",
              "HierarchyPath": "SAGIN > AI Optimization > DRL",
              "Concepts": ["Deep Reinforcement Learning", "MDP Formulation", "Resource Allocation", "Network Testbed"],
              "KeyClaims": [
                "DRL eliminates need for explicit mathematical network models",
                "MDP formulation captures network dynamics, actions, and rewards",
                "23% throughput improvement over weighted round-robin baseline",
                "31% packet loss reduction in dynamic traffic scenarios",
                "Testbed includes 6 LEO satellites, 8 UAVs, 12 ground stations"
              ],
              "QuestionsRaised": [
                "How does DRL performance scale with increasing network size?",
                "What is the computational overhead of real-time DRL inference?"
              ],
              "EvidenceStrength": 0.94,
              "StartPos": 12830,
              "EndPos": 14215,
              "ChunkIndex": 28,
              "HasMore": true,
              "OverlapLength": 180
            },
            {
              "Id": "evidence-drl-overview-kr001",
              "SourceId": "pdf-energy-aware-sagin",
              "SourceName": "Energy Aware Scheduling and Optimization for Image Delivery in Space-Air-Ground Integrated Networks",
              "ChunkId": "chunk-022",
              "Text": "에너지 제약 SAGIN 환경에서 이미지 전송 스케줄링은 복잡한 최적화 문제이다. 위성의 제한된 에너지 예산, 동적 채널 조건, 다양한 이미지 우선순위를 동시에 고려해야 한다. 우리는 Dueling DQN 기반 접근법을 제안하여 이 문제를 해결한다. 시뮬레이션 환경은 10개 LEO 위성 (궤도 고도: 600km, 주기: 96분), 10개 UAV (고도: 1-3km, 체공 시간: 4시간), 15개 지상국을 포함한다. 각 위성은 초기 에너지 용량 5000 Wh를 가지며, 이미지 전송 시 패킷당 0.8-1.5 W를 소비한다. 실험 결과, 제안된 DRL 방법은 기존 greedy 스케줄링 대비 평균 18% 에너지 절감을 달성하면서 95% 전송 성공률을 유지했다. 특히 고우선순위 긴급 이미지의 경우 평균 전송 지연이 42% 감소했다.",
              "Page": 4,
              "Confidence": 0.92,
              "CreatedAt": "2025-01-15T09:05:00Z",
              "Language": "ENG",
              "SourceLanguage": "KOR",
              "HierarchyPath": "SAGIN > AI Optimization > DRL",
              "Concepts": ["Energy-Constrained Scheduling", "Image Delivery", "Dueling DQN", "Priority-Based QoS"],
              "KeyClaims": [
                "SAGIN image delivery requires balancing energy, channel conditions, and priority",
                "Testbed includes 10 LEO satellites, 10 UAVs, 15 ground stations",
                "Satellites have 5000 Wh capacity, consume 0.8-1.5 W per packet",
                "18% energy reduction vs greedy scheduling with 95% success rate",
                "42% latency reduction for high-priority emergency images"
              ],
              "QuestionsRaised": [
                "How to handle energy replenishment through solar panels in the model?",
                "What is the impact of cloud cover on UAV-ground links in the scheduling decision?"
              ],
              "EvidenceStrength": 0.90,
              "StartPos": 9450,
              "EndPos": 10720,
              "ChunkIndex": 22,
              "HasMore": true,
              "OverlapLength": 160
            }
          ],
          "GapSuggestions": []
        },
        {
          "Id": "concept-marl001",
          "Type": "concept",
          "Name": "Multi-Agent Reinforcement Learning for Decentralized Control",
          "Synthesis": "MARL enables distributed decision-making across SAGIN nodes without requiring centralized coordination. The English paper proposes an Independent DQN (IDQN) approach where each satellite and UAV acts as an autonomous agent, learning policies based on local observations. Communication overhead is reduced by 67% compared to centralized methods while achieving comparable performance in scenarios with up to 20 agents.",
          "WorkspaceId": "Reinforcement-Learning",
          "Level": 2,
          "SourceCount": 1,
          "TotalConfidence": 0.88,
          "CreatedAt": "2025-01-15T10:00:00Z",
          "UpdatedAt": "2025-01-15T10:00:00Z",
          "Children": [
            {
              "Id": "subconcept-idqn001",
              "Type": "subconcept",
              "Name": "Independent DQN for Satellite-UAV Coordination",
              "Synthesis": "Each satellite and UAV maintains its own DQN agent that learns optimal transmission policies based on partial observations of the network. Agents share a common reward structure (network-wide throughput) but make independent decisions. The approach handles non-stationarity through target network stabilization and gradual exploration decay (ε: 1.0 → 0.01 over 50k steps).",
              "WorkspaceId": "Reinforcement-Learning",
              "Level": 3,
              "SourceCount": 1,
              "TotalConfidence": 0.87,
              "CreatedAt": "2025-01-15T10:15:00Z",
              "UpdatedAt": "2025-01-15T10:15:00Z",
              "Children": [],
              "Evidences": [
                {
                  "Id": "evidence-idqn-en001",
                  "SourceId": "pdf-optimizing-sagin-ai",
                  "SourceName": "Optimizing Space-Air-Ground Integrated Networks by Artificial Intelligence",
                  "ChunkId": "chunk-056",
                  "Text": "To address scalability challenges in large-scale SAGIN, we propose a decentralized multi-agent approach using Independent DQN (IDQN). Each satellite (6 LEO agents) and UAV (8 aerial agents) operates as an autonomous learning agent with its own neural network. The local observation space for each satellite agent includes: (i) its own residual energy and solar panel status, (ii) visible ground stations and their signal strengths, (iii) onboard buffer status for different traffic classes, (iv) current time slot and predicted visibility windows. UAV agents observe similar features plus their 3D position, heading, and battery depletion rate. All agents share a common global reward signal computed as R_t = α·throughput + β·fairness - γ·energy_cost, with weights α=0.5, β=0.3, γ=0.2. Despite non-stationarity from concurrent learning of multiple agents, we achieve convergence by employing target networks updated every 2000 steps and gradual ε-decay from 1.0 to 0.01 over 50,000 training episodes. Communication overhead analysis shows IDQN requires only 3.2 KB/s control signaling compared to 9.8 KB/s for centralized DQN with 14 agents, representing a 67% reduction. Performance metrics indicate IDQN achieves 94% of centralized optimal throughput while enabling truly distributed operation.",
                  "Page": 9,
                  "Confidence": 0.87,
                  "CreatedAt": "2025-01-15T10:15:00Z",
                  "Language": "ENG",
                  "SourceLanguage": "ENG",
                  "HierarchyPath": "SAGIN > AI Optimization > MARL > IDQN",
                  "Concepts": ["Independent DQN", "Decentralized Learning", "Multi-Agent Systems", "Communication Overhead"],
                  "KeyClaims": [
                    "Each satellite and UAV operates as autonomous learning agent",
                    "Local observations include energy, visibility, buffer, timing info",
                    "Global reward balances throughput, fairness, and energy cost",
                    "67% communication overhead reduction vs centralized approach",
                    "Achieves 94% of centralized optimal performance",
                    "Target network updates every 2000 steps enable convergence"
                  ],
                  "QuestionsRaised": [
                    "How to handle credit assignment in cooperative multi-agent scenarios?",
                    "Can value decomposition methods (QMIX, QTRAN) improve coordination?"
                  ],
                  "EvidenceStrength": 0.86,
                  "StartPos": 23680,
                  "EndPos": 25210,
                  "ChunkIndex": 56,
                  "HasMore": true,
                  "OverlapLength": 175
                }
              ],
              "GapSuggestions": [
                {
                  "Id": "gap-marl-comm001",
                  "SuggestionText": "Explore CommNet or TarMAC architectures to enable learned communication protocols between SAGIN agents for improved coordination",
                  "TargetNodeId": "https://arxiv.org/abs/1605.07736",
                  "TargetFileId": "",
                  "SimilarityScore": 0.79
                },
                {
                  "Id": "gap-marl-qmix001",
                  "SuggestionText": "Investigate QMIX or QTRAN for credit assignment in cooperative satellite-UAV task completion scenarios",
                  "TargetNodeId": "https://arxiv.org/abs/1803.11485",
                  "TargetFileId": "",
                  "SimilarityScore": 0.81
                }
              ]
            }
          ],
          "Evidences": [
            {
              "Id": "evidence-marl-overview-en001",
              "SourceId": "pdf-optimizing-sagin-ai",
              "SourceName": "Optimizing Space-Air-Ground Integrated Networks by Artificial Intelligence",
              "ChunkId": "chunk-052",
              "Text": "Centralized control becomes impractical in large-scale SAGIN deployments due to communication delays (satellite-ground RTT: 25-40 ms), bandwidth constraints, and single-point-of-failure risks. Multi-Agent Reinforcement Learning (MARL) offers a compelling alternative through distributed intelligence. We investigate three MARL paradigms: (1) Independent learners where each agent treats others as part of the environment, (2) Centralized training with decentralized execution (CTDE) where agents share information during training but operate independently at deployment, (3) Fully cooperative approaches with explicit coordination mechanisms. Our analysis focuses on independent DQN as it requires minimal communication infrastructure and naturally handles agent heterogeneity (satellites vs UAVs have different capabilities and constraints). Scalability experiments demonstrate that IDQN maintains stable performance up to 20 agents, whereas centralized approaches suffer exponential action space growth. The key challenge in MARL for SAGIN is non-stationarity: from each agent's perspective, the environment dynamics change as other agents simultaneously learn and update their policies.",
              "Page": 9,
              "Confidence": 0.88,
              "CreatedAt": "2025-01-15T10:00:00Z",
              "Language": "ENG",
              "SourceLanguage": "ENG",
              "HierarchyPath": "SAGIN > AI Optimization > MARL",
              "Concepts": ["Multi-Agent RL", "Decentralized Control", "Scalability", "Non-Stationarity"],
              "KeyClaims": [
                "Centralized control impractical due to 25-40ms satellite-ground RTT",
                "Three MARL paradigms evaluated: independent, CTDE, fully cooperative",
                "IDQN scales to 20 agents with stable performance",
                "Non-stationarity is key challenge as all agents learn concurrently"
              ],
              "QuestionsRaised": [
                "How to quantify the degree of non-stationarity in SAGIN MARL?",
                "What are optimal training curriculum strategies for MARL convergence?"
              ],
              "EvidenceStrength": 0.87,
              "StartPos": 22140,
              "EndPos": 23450,
              "ChunkIndex": 52,
              "HasMore": true,
              "OverlapLength": 165
            }
          ],
          "GapSuggestions": []
        }
      ],
      "Evidences": [
        {
          "Id": "evidence-aiopt-intro-en001",
          "SourceId": "pdf-optimizing-sagin-ai",
          "SourceName": "Optimizing Space-Air-Ground Integrated Networks by Artificial Intelligence",
          "ChunkId": "chunk-015",
          "Text": "Traditional optimization approaches for SAGIN face fundamental limitations when dealing with the scale and dynamics of modern deployments. Linear programming and convex optimization methods assume static or slowly-varying channel conditions, which is unrealistic given LEO satellite orbital periods (90-120 minutes) and UAV mobility patterns (velocities: 10-25 m/s). Heuristic algorithms (greedy, genetic algorithms, particle swarm optimization) lack adaptability and require extensive domain knowledge for parameter tuning. In contrast, AI-driven methods, particularly Deep Reinforcement Learning, can learn near-optimal policies directly from network observations without explicit modeling. Our work systematically explores DRL and MARL techniques across three key SAGIN optimization domains: (1) dynamic resource allocation across heterogeneous links, (2) intelligent handover management during satellite visibility gaps, and (3) energy-aware traffic scheduling considering solar energy harvesting cycles. We benchmark AI methods against state-of-the-art baselines including MILP solvers (limited to small problem instances) and domain-specific heuristics.",
          "Page": 3,
          "Confidence": 0.94,
          "CreatedAt": "2025-01-15T08:35:00Z",
          "Language": "ENG",
          "SourceLanguage": "ENG",
          "HierarchyPath": "SAGIN > AI Optimization",
          "Concepts": ["AI Optimization", "DRL", "MARL", "Traditional Methods Limitations"],
          "KeyClaims": [
            "Traditional methods assume static conditions, unrealistic for LEO/UAV dynamics",
            "Heuristics lack adaptability and require extensive parameter tuning",
            "DRL learns policies from observations without explicit modeling",
            "Three key domains: resource allocation, handover, energy scheduling",
            "Benchmarked against MILP solvers and domain heuristics"
          ],
          "QuestionsRaised": [
            "What is the sample complexity of DRL methods for SAGIN problems?",
            "How to ensure safety constraints during online RL exploration?"
          ],
          "EvidenceStrength": 0.92,
          "StartPos": 6830,
          "EndPos": 8245,
          "ChunkIndex": 15,
          "HasMore": true,
          "OverlapLength": 140
        },
        {
          "Id": "evidence-aiopt-intro-kr001",
          "SourceId": "pdf-energy-aware-sagin",
          "SourceName": "Energy Aware Scheduling and Optimization for Image Delivery in Space-Air-Ground Integrated Networks",
          "ChunkId": "chunk-012",
          "Text": "위성 네트워크에서 이미지 전송은 에너지 제약, 불규칙한 통신 기회, 다양한 QoS 요구사항으로 인해 복잡하다. 기존 연구는 주로 단일 계층 최적화에 집중했으나, SAGIN의 진정한 이점은 계층 간 협력적 스케줄링에 있다. 우리는 인공지능, 특히 심층 강화학습을 활용하여 위성-UAV-지상 계층에 걸친 통합 이미지 전송 스케줄링 문제를 해결한다. 제안된 프레임워크는 세 가지 핵심 요소를 포함한다: (1) 에너지 인식 상태 표현 - 위성 배터리 잔량, 태양 전지판 각도, 예상 에너지 수확량, (2) 멀티 홉 경로 선택 - 위성 직접 전송 vs UAV 중계 vs 지상 백홀 우회, (3) 우선순위 기반 보상 설계 - 긴급 이미지는 1.5x 가중치, 일반 이미지는 1.0x, 저우선순위는 0.7x. 시뮬레이션 결과 Dueling DQN 접근법이 rule-based 스케줄러 대비 에너지 효율성 18% 향상, 평균 전송 지연 28% 감소를 달성했다.",
          "Page": 3,
          "Confidence": 0.91,
          "CreatedAt": "2025-01-15T08:40:00Z",
          "Language": "ENG",
          "SourceLanguage": "KOR",
          "HierarchyPath": "SAGIN > AI Optimization",
          "Concepts": ["Energy-Aware Scheduling", "Cross-Layer Optimization", "Priority-Based QoS", "Multi-Hop Routing"],
          "KeyClaims": [
            "SAGIN benefits emerge from cross-layer cooperative scheduling",
            "Energy-aware state includes battery, solar panel angle, harvest prediction",
            "Multi-hop routing considers satellite direct, UAV relay, ground backhaul",
            "Priority-based rewards: emergency 1.5x, normal 1.0x, low 0.7x",
            "18% energy efficiency improvement, 28% latency reduction vs rule-based"
          ],
          "QuestionsRaised": [
            "How to model stochastic solar energy harvesting in polar regions?",
            "What is the optimal trade-off between transmission delay and energy cost?"
          ],
          "EvidenceStrength": 0.89,
          "StartPos": 5240,
          "EndPos": 6680,
          "ChunkIndex": 12,
          "HasMore": true,
          "OverlapLength": 155
        }
      ],
      "GapSuggestions": []
    },
    {
      "Id": "category-energy001",
      "Type": "category",
      "Name": "Energy Management in SAGIN",
      "Synthesis": "Energy management is critical in SAGIN due to limited battery capacities of satellites and UAVs, intermittent solar energy harvesting, and dynamic power consumption patterns. The Korean paper focuses extensively on energy-aware scheduling strategies that optimize transmission timing based on predicted energy availability, while the English paper addresses energy-efficient routing that minimizes transmission power through intelligent relay selection.",
      "WorkspaceId": "Reinforcement-Learning",
      "Level": 1,
      "SourceCount": 2,
      "TotalConfidence": 0.91,
      "CreatedAt": "2025-01-15T11:00:00Z",
      "UpdatedAt": "2025-01-15T14:22:00Z",
      "Children": [
        {
          "Id": "concept-battery001",
          "Type": "concept",
          "Name": "Battery-Aware Transmission Scheduling",
          "Synthesis": "Satellites and UAVs must balance immediate transmission demands against future energy needs. The Korean paper proposes a prediction-based scheduling framework that estimates future energy harvesting (using orbital mechanics and historical solar irradiance data) and defers non-critical transmissions to energy-abundant periods. This approach extends mission lifetime by 34% in winter deployment scenarios.",
          "WorkspaceId": "Reinforcement-Learning",
          "Level": 2,
          "SourceCount": 1,
          "TotalConfidence": 0.89,
          "CreatedAt": "2025-01-15T11:15:00Z",
          "UpdatedAt": "2025-01-15T11:15:00Z",
          "Children": [
            {
              "Id": "subconcept-energypred001",
              "Type": "subconcept",
              "Name": "Solar Energy Harvesting Prediction",
              "Synthesis": "Accurate prediction of solar energy harvesting is crucial for proactive scheduling. The model considers satellite orbital position, solar panel orientation angle, Earth shadow periods, and seasonal variations. A hybrid LSTM-based predictor achieves 92% accuracy for 30-minute ahead predictions and 78% accuracy for 2-hour predictions.",
              "WorkspaceId": "Reinforcement-Learning",
              "Level": 3,
              "SourceCount": 1,
              "TotalConfidence": 0.87,
              "CreatedAt": "2025-01-15T11:30:00Z",
              "UpdatedAt": "2025-01-15T11:30:00Z",
              "Children": [],
              "Evidences": [
                {
                  "Id": "evidence-solarpred-kr001",
                  "SourceId": "pdf-energy-aware-sagin",
                  "SourceName": "Energy Aware Scheduling and Optimization for Image Delivery in Space-Air-Ground Integrated Networks",
                  "ChunkId": "chunk-045",
                  "Text": "태양 에너지 수확 예측은 사전 스케줄링의 핵심이다. 우리는 LSTM 기반 예측 모델을 제안한다. 입력 특징은 다음을 포함한다: (i) 위성의 현재 궤도 위치 (경도, 위도, 고도), (ii) 태양 전지판 방향 각도 (α, β), (iii) 지구 그림자 진입/탈출 예상 시간, (iv) 계절별 태양 복사 강도 (겨울: 평균 850 W/m², 여름: 평균 1100 W/m²), (v) 과거 30분간 실제 수확 에너지. LSTM 네트워크는 2개 레이어 (hidden size: 128)로 구성되며, 30분 앞선 에너지 수확량을 예측한다. 6개월 궤도 데이터 (총 432,000개 시간 스텝)로 학습한 결과, 30분 예측의 평균 절대 오차(MAE)는 8.2%, 2시간 예측은 22.1%였다. 예측 정확도는 에너지 인식 스케줄러의 성능에 직접적 영향을 미친다. 정확한 예측 시 불필요한 전송 지연을 34% 감소시키고, 에너지 고갈로 인한 전송 실패를 67% 줄였다.",
                  "Page": 8,
                  "Confidence": 0.87,
                  "CreatedAt": "2025-01-15T11:30:00Z",
                  "Language": "ENG",
                  "SourceLanguage": "KOR",
                  "HierarchyPath": "SAGIN > Energy Management > Battery-Aware Scheduling > Solar Prediction",
                  "Concepts": ["LSTM", "Energy Harvesting", "Time Series Prediction", "Orbital Mechanics"],
                  "KeyClaims": [
                    "LSTM predictor uses orbital position, panel angle, shadow periods, seasonal variations",
                    "2-layer LSTM with hidden size 128 trained on 432k timesteps",
                    "30-min prediction MAE: 8.2%, 2-hour prediction MAE: 22.1%",
                    "Accurate prediction reduces unnecessary delays by 34%",
                    "Transmission failures due to energy depletion reduced by 67%"
                  ],
                  "QuestionsRaised": [
                    "How to adapt predictions during unexpected solar storms?",
                    "Can transfer learning improve predictions for newly deployed satellites?"
                  ],
                  "EvidenceStrength": 0.85,
                  "StartPos": 19320,
                  "EndPos": 20785,
                  "ChunkIndex": 45,
                  "HasMore": true,
                  "OverlapLength": 170
                }
              ],
              "GapSuggestions": [
                {
                  "Id": "gap-energy-transformer001",
                  "SuggestionText": "Explore Transformer-based models with attention mechanisms for better capturing long-term seasonal patterns in solar energy harvesting",
                  "TargetNodeId": "https://arxiv.org/abs/2012.07436",
                  "TargetFileId": "",
                  "SimilarityScore": 0.76
                }
              ]
            }
          ],
          "Evidences": [
            {
              "Id": "evidence-battery-kr001",
              "SourceId": "pdf-energy-aware-sagin",
              "SourceName": "Energy Aware Scheduling and Optimization for Image Delivery in Space-Air-Ground Integrated Networks",
              "ChunkId": "chunk-041",
              "Text": "위성과 UAV의 배터리 용량은 제한적이다. LEO 위성은 일반적으로 5000-7000 Wh 배터리를 탑재하며, 지구 그림자 구간(약 35분)에는 태양 에너지 수확이 불가능하다. UAV는 더 제한적으로 200-500 Wh 배터리로 4-6시간 체공한다. 배터리 인식 스케줄링은 즉각적 전송 수요와 미래 에너지 필요 사이의 균형을 맞춘다. 우리의 접근법은 세 가지 전략을 사용한다: (1) 예측 기반 지연 - 태양 에너지 수확 예측을 사용하여 비긴급 이미지 전송을 에너지 풍부 기간으로 연기 (2) 동적 전력 제어 - 배터리 수준에 따라 전송 전력 조정 (배터리 > 70%: 최대 전력, 30-70%: 중간 전력, < 30%: 긴급 전송만), (3) 협력적 오프로딩 - 에너지 부족 위성은 인근 UAV나 다른 위성에 전송 작업 위임. 겨울 시나리오 시뮬레이션 (태양 복사 감소 30%)에서 배터리 인식 스케줄링은 기준선 대비 임무 수명을 34% 연장했다.",
              "Page": 7,
              "Confidence": 0.89,
              "CreatedAt": "2025-01-15T11:15:00Z",
              "Language": "ENG",
              "SourceLanguage": "KOR",
              "HierarchyPath": "SAGIN > Energy Management > Battery-Aware Scheduling",
              "Concepts": ["Battery Management", "Energy Prediction", "Dynamic Power Control", "Task Offloading"],
              "KeyClaims": [
                "LEO satellites: 5000-7000 Wh battery, 35-min shadow period with no harvesting",
                "UAVs: 200-500 Wh battery, 4-6 hour endurance",
                "Three strategies: predictive deferral, dynamic power control, cooperative offloading",
                "Power levels adjusted based on battery: >70% max, 30-70% medium, <30% emergency only",
                "34% mission lifetime extension in winter scenarios with 30% reduced solar radiation"
              ],
              "QuestionsRaised": [
                "How to coordinate offloading decisions among multiple energy-constrained nodes?",
                "What is the impact of battery degradation over mission lifetime on scheduling?"
              ],
              "EvidenceStrength": 0.88,
              "StartPos": 17640,
              "EndPos": 19085,
              "ChunkIndex": 41,
              "HasMore": true,
              "OverlapLength": 165
            }
          ],
          "GapSuggestions": []
        },
        {
          "Id": "concept-powercontrol001",
          "Type": "concept",
          "Name": "Adaptive Power Control for Energy Efficiency",
          "Synthesis": "Transmission power directly impacts both link quality and energy consumption. The English paper proposes an adaptive power control scheme where DRL agents learn to select optimal power levels (5 discrete levels: 0.5W, 1W, 2W, 5W, 10W) based on channel conditions and energy constraints. Results show 41% energy savings with only 3% throughput degradation compared to fixed maximum power transmission.",
          "WorkspaceId": "Reinforcement-Learning",
          "Level": 2,
          "SourceCount": 1,
          "TotalConfidence": 0.86,
          "CreatedAt": "2025-01-15T12:00:00Z",
          "UpdatedAt": "2025-01-15T12:00:00Z",
          "Children": [],
          "Evidences": [
            {
              "Id": "evidence-powerctrl-en001",
              "SourceId": "pdf-optimizing-sagin-ai",
              "SourceName": "Optimizing Space-Air-Ground Integrated Networks by Artificial Intelligence",
              "ChunkId": "chunk-068",
              "Text": "Adaptive power control is essential for energy-efficient SAGIN operation. We formulate power control as part of the DRL action space, allowing agents to jointly optimize transmission power and resource allocation. The power action space consists of 5 discrete levels: P ∈ {0.5, 1, 2, 5, 10} Watts. State features include: (i) instantaneous channel gain h(t) measured via pilot signals, (ii) interference levels from co-channel transmissions, (iii) residual energy E_residual, (iv) buffer occupancy and traffic urgency, (v) link budget requirements for target SNR. The reward function incorporates an energy penalty term: R_t = throughput(t) - λ·P(t), where λ=0.15 is the energy cost coefficient calibrated to satellite solar panel capacity (50W average). Our DQN agent learns to reduce power during favorable channel conditions (high h(t)) and increase power only when necessary to maintain QoS. Experimental results across 10,000 orbital periods demonstrate 41% average energy reduction compared to fixed maximum power baseline, with throughput degradation limited to 3.2%. Power level distribution analysis reveals the agent predominantly selects 1-2W (68% of decisions) with occasional boosts to 5-10W during deep fading (12% of decisions).",
              "Page": 11,
              "Confidence": 0.86,
              "CreatedAt": "2025-01-15T12:00:00Z",
              "Language": "ENG",
              "SourceLanguage": "ENG",
              "HierarchyPath": "SAGIN > Energy Management > Adaptive Power Control",
              "Concepts": ["Power Control", "Energy Efficiency", "Channel-Aware Transmission", "Joint Optimization"],
              "KeyClaims": [
                "5 discrete power levels: 0.5W, 1W, 2W, 5W, 10W",
                "State includes channel gain, interference, residual energy, buffer, link budget",
                "Reward includes energy penalty: R = throughput - 0.15·P",
                "41% energy reduction with only 3.2% throughput degradation",
                "Agent selects 1-2W for 68% of decisions, 5-10W for deep fading 12%"
              ],
              "QuestionsRaised": [
                "How to extend to continuous power control with DDPG or SAC?",
                "What is the optimal energy cost coefficient λ for different satellite types?"
              ],
              "EvidenceStrength": 0.85,
              "StartPos": 28950,
              "EndPos": 30620,
              "ChunkIndex": 68,
              "HasMore": true,
              "OverlapLength": 185
            }
          ],
          "GapSuggestions": [
            {
              "Id": "gap-power-continuous001",
              "SuggestionText": "Investigate continuous power control using actor-critic methods (DDPG, TD3, SAC) for finer-grained optimization and potentially better energy-throughput trade-offs",
              "TargetNodeId": "https://arxiv.org/abs/1509.02971",
              "TargetFileId": "",
              "SimilarityScore": 0.83
            },
            {
              "Id": "gap-power-robust001",
              "SuggestionText": "Explore robust power control under channel uncertainty using distributional RL or risk-sensitive formulations",
              "TargetNodeId": "https://arxiv.org/abs/1707.06887",
              "TargetFileId": "",
              "SimilarityScore": 0.77
            }
          ]
        }
      ],
      "Evidences": [
        {
          "Id": "evidence-energy-intro-kr001",
          "SourceId": "pdf-energy-aware-sagin",
          "SourceName": "Energy Aware Scheduling and Optimization for Image Delivery in Space-Air-Ground Integrated Networks",
          "ChunkId": "chunk-008",
          "Text": "SAGIN에서 에너지 관리는 시스템 지속 가능성의 핵심이다. 위성은 제한된 배터리 용량과 간헐적 태양 에너지 수확에 의존한다. LEO 위성은 96분 궤도 주기 중 약 60분은 햇빛을 받고 35분은 지구 그림자에 있다. UAV는 더욱 제한적이며 일반적으로 4-6시간 체공 시간을 가진다. 에너지 소비는 전송 전력, 데이터 속도, 회로 전력에 의해 결정된다. 위성 전송은 패킷당 0.8-1.5W를 소비하며, UAV는 이동과 호버링에도 상당한 전력(평균 150W)을 사용한다. 에너지 인식 최적화는 다음 목표를 추구한다: (1) 전송 기회 최대화 - 에너지 제약 내에서, (2) 임무 수명 연장 - 장기 에너지 균형 유지, (3) QoS 보장 - 우선순위 트래픽의 적시 전송. 기존 연구는 주로 단기 에너지 최소화에 집중했지만, 우리는 장기 에너지-성능 trade-off를 다루는 심층 강화학습 접근법을 제시한다.",
          "Page": 2,
          "Confidence": 0.91,
          "CreatedAt": "2025-01-15T11:00:00Z",
          "Language": "ENG",
          "SourceLanguage": "KOR",
          "HierarchyPath": "SAGIN > Energy Management",
          "Concepts": ["Energy Sustainability", "Battery Constraints", "Solar Harvesting", "Energy-Performance Tradeoff"],
          "KeyClaims": [
            "LEO satellites: 60-min sunlight, 35-min shadow in 96-min orbit",
            "UAVs: 4-6 hour endurance, average 150W for mobility and hovering",
            "Satellite transmission: 0.8-1.5W per packet",
            "Three objectives: maximize transmission opportunities, extend mission lifetime, guarantee QoS",
            "DRL addresses long-term energy-performance trade-offs vs short-term minimization"
          ],
          "QuestionsRaised": [
            "How to model energy harvesting uncertainty in RL reward design?",
            "What is the optimal exploration strategy to avoid energy-critical states?"
          ],
          "EvidenceStrength": 0.90,
          "StartPos": 3180,
          "EndPos": 4825,
          "ChunkIndex": 8,
          "HasMore": true,
          "OverlapLength": 145
        },
        {
          "Id": "evidence-energy-intro-en001",
          "SourceId": "pdf-optimizing-sagin-ai",
          "SourceName": "Optimizing Space-Air-Ground Integrated Networks by Artificial Intelligence",
          "ChunkId": "chunk-062",
          "Text": "Energy efficiency is paramount in SAGIN due to the reliance on battery-powered satellites and UAVs with limited energy replenishment opportunities. Our work addresses energy management through two complementary approaches: (1) energy-efficient routing that selects paths minimizing total transmission energy while meeting delay constraints, (2) adaptive power control that adjusts transmission power based on channel conditions and energy availability. We model energy consumption using realistic power models: satellite transmission power P_tx = 0.5-10W (depending on distance and data rate), circuit power P_circuit = 2W, solar panel harvesting rate = 40-50W (in sunlight), battery capacity C_bat = 5000-7000 Wh. For UAVs: propulsion power P_prop = 100-200W (speed-dependent), hovering power P_hover = 150W, transmission power P_tx = 0.5-5W, battery capacity C_bat = 300-500 Wh. The challenge is balancing immediate performance needs (throughput, latency) against long-term energy sustainability. DRL agents learn value functions that implicitly capture this trade-off through appropriately designed reward structures.",
          "Page": 10,
          "Confidence": 0.88,
          "CreatedAt": "2025-01-15T11:05:00Z",
          "Language": "ENG",
          "SourceLanguage": "ENG",
          "HierarchyPath": "SAGIN > Energy Management",
          "Concepts": ["Energy Efficiency", "Routing Optimization", "Power Models", "Energy-Performance Trade-off"],
          "KeyClaims": [
            "Two approaches: energy-efficient routing and adaptive power control",
            "Satellite: 0.5-10W transmission, 2W circuit, 40-50W harvesting, 5000-7000 Wh battery",
            "UAV: 100-200W propulsion, 150W hovering, 0.5-5W transmission, 300-500 Wh battery",
            "DRL learns implicit energy-performance trade-off through reward design"
          ],
          "QuestionsRaised": [
            "How to incorporate battery degradation models in long-term RL planning?",
            "Can meta-learning enable fast adaptation to varying energy availability patterns?"
          ],
          "EvidenceStrength": 0.87,
          "StartPos": 26340,
          "EndPos": 27895,
          "ChunkIndex": 62,
          "HasMore": true,
          "OverlapLength": 160
        }
      ],
      "GapSuggestions": []
    }
  ],
  "Evidences": [
    {
      "Id": "evidence-domain-en001",
      "SourceId": "pdf-optimizing-sagin-ai",
      "SourceName": "Optimizing Space-Air-Ground Integrated Networks by Artificial Intelligence",
      "ChunkId": "chunk-005",
      "Text": "Space-Air-Ground Integrated Networks (SAGIN) represent a paradigm shift in communication infrastructure, seamlessly integrating satellite constellations (GEO, MEO, LEO), aerial platforms (UAVs, HAPs), and terrestrial networks to provide global coverage and ubiquitous connectivity. The heterogeneous nature of SAGIN introduces unique challenges including diverse propagation characteristics (satellite: free-space path loss with atmospheric attenuation, aerial: LoS-dominant with mobility effects, terrestrial: multi-path fading), varying latency constraints (satellite: 20-500ms, aerial: 10-50ms, terrestrial: <10ms), and disparate energy availability profiles. Modern SAGIN architectures typically comprise 6-12 LEO satellites per orbital plane at 500-1200 km altitude, 5-20 UAVs operating at 1-5 km altitude with 4-8 hour endurance, and dense terrestrial base stations. Key performance metrics include end-to-end latency (<100ms for real-time applications), reliability (99.9% for critical services), and energy efficiency (bits/Joule across all tiers).",
      "Page": 1,
      "Confidence": 0.92,
      "CreatedAt": "2025-01-15T08:30:00Z",
      "Language": "ENG",
      "SourceLanguage": "ENG",
      "HierarchyPath": "SAGIN",
      "Concepts": ["Network Architecture", "Heterogeneous Networks", "Propagation Characteristics", "Performance Metrics"],
      "KeyClaims": [
        "SAGIN integrates satellite, aerial, and terrestrial networks for global coverage",
        "Diverse propagation: satellite (free-space + atmospheric), aerial (LoS + mobility), terrestrial (multi-path)",
        "Latency constraints: satellite 20-500ms, aerial 10-50ms, terrestrial <10ms",
        "Typical deployment: 6-12 LEO satellites, 5-20 UAVs, dense terrestrial base stations",
        "Key metrics: latency <100ms, reliability 99.9%, energy efficiency bits/Joule"
      ],
      "QuestionsRaised": [
        "How to standardize protocols across such heterogeneous network tiers?",
        "What are the optimal ratios of satellite/aerial/terrestrial nodes for different geographic regions?"
      ],
      "EvidenceStrength": 0.91,
      "StartPos": 1850,
      "EndPos": 3420,
      "ChunkIndex": 5,
      "HasMore": true,
      "OverlapLength": 120
    },
    {
      "Id": "evidence-domain-kr001",
      "SourceId": "pdf-energy-aware-sagin",
      "SourceName": "Energy Aware Scheduling and Optimization for Image Delivery in Space-Air-Ground Integrated Networks",
      "ChunkId": "chunk-003",
      "Text": "SAGIN(우주-공중-지상 통합 네트워크)은 위성 네트워크, 공중 플랫폼(UAV, HAP), 지상 인프라의 통합을 통해 전 지구적 커버리지와 유비쿼터스 연결성을 제공한다. 이러한 3계층 아키텍처는 각기 다른 특성을 가진다: 위성 계층은 광범위한 커버리지(지름 1000-3000km) but 높은 지연(25-40ms), 공중 계층은 유연한 배치 but 제한된 체공 시간, 지상 계층은 높은 대역폭 but 제한된 커버리지. SAGIN의 주요 응용 분야는 원격 감지, 재난 통신, 인터넷 접근 서비스, 군사 작전을 포함한다. 우리의 연구는 특히 에너지 제약 환경에서 이미지 전송에 초점을 맞추며, 위성의 태양 에너지 수확 주기, UAV의 배터리 제약, 지상 스테이션의 전력 가용성을 통합적으로 고려한다.",
      "Page": 1,
      "Confidence": 0.91,
      "CreatedAt": "2025-01-15T08:32:00Z",
      "Language": "ENG",
      "SourceLanguage": "KOR",
      "HierarchyPath": "SAGIN",
      "Concepts": ["Three-Tier Architecture", "Global Coverage", "Energy Constraints", "Image Delivery Applications"],
      "KeyClaims": [
        "Three-tier architecture: satellite (wide coverage, high latency), aerial (flexible deployment, limited endurance), terrestrial (high bandwidth, limited coverage)",
        "Satellite coverage diameter: 1000-3000km with 25-40ms latency",
        "Key applications: remote sensing, disaster communications, internet access, military operations",
        "Focus on energy-constrained image delivery considering solar harvesting, battery limits, power availability"
      ],
      "QuestionsRaised": [
        "How to prioritize different applications with conflicting QoS requirements?",
        "What are the optimal energy management strategies across the three tiers?"
      ],
      "EvidenceStrength": 0.90,
      "StartPos": 950,
      "EndPos": 2280,
      "ChunkIndex": 3,
      "HasMore": true,
      "OverlapLength": 130
    }
  ],
  "GapSuggestions": []
}